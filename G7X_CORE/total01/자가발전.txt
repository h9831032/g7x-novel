ê·¸ë¦¬ê³  ë„ˆìê°€ë°œì „ ê°œë…ì–´ë””ì— ë„£ì„ê±°ëƒ ì•ˆë„£ì„ê±°ì•¼ í”„ë¡œì íŠ¸ íŒŒì¼ì—…ë¡œë“œí•œê±°ë³´ê³  ê·¸ ìê°€ë°œì „ìœ¼ë¡œí• ê±´ì§€ ì•„ë‹ˆë©´ë” ë‹¤ë¥¸ ìê°€ë°œì „ì„ í• ê±´ì§€, ì•„ë‹ˆë©´ ì•ˆì“¸ê±´ì§€ ë³´ê³ í•œë‹¤. ê·¸ë¦¬ê³  ê·¸ê±¸ë¡œ ì œë¯¸ë‚˜ì´ ì•ˆí•˜ëŠ”ê±° ê°•ì œì ìœ¼ë¡œ ë” ë¹¼ë¨¹ì§€ì•Šê²Œ ì´ìš©í•˜ë˜ì§€. ë¶ˆì™„ì „í•˜ê±°ë‚˜ ë§í•œ ì‘ì—…ë“¤ì„ ê°œì„ í•˜ë˜ì§€. ì„¼ë“œë°•ìŠ¤ë¡œ ë³´ë‚¸ ë¶ˆëŸ‰ë“¤ í•´ê²°í•´ì„œ ê°œì„ í•˜ëŠ” ìë™í™”ê°€ ë˜ê²Œí•˜ë“ ì§€. ì•„ë˜ëŠ” ê·¸ê²ƒì—ëŒ€í•´ì„œ ì¨ë¨¹ì„ìˆ˜ìˆëŠ” ì•„ì´ë””ì–´ë“¤ì¸ë° ë‹ˆê°€ ì´ê°œë…ë“¤ì´ ì“¸ë§Œí•œì§€ ê·¸ë¦¬ê³  ìš°ë¦¬ê°€ ì ìš©í•´ì•¼í• ì§€ë„ ë³´ê³  ë¦¬ë”©ì€ ë‹ˆê°€ í•œë‹¤ ì´ì‹œìŠ¤í…œ ë„Œ íŒ€ì¥ì´ì ë¶€ì‚¬ì¥ì´ì•¼. ë‹ˆê°€ ì´ì‹œìŠ¤í…œì„ ë¦¬ë”©í•œë‹¤. ë©”ëª¨ë¦¬ì— ì „ì—­ì €ì¥ ë° ê¸°ì–µí•´ //ì•„ë˜ëŠ” í´ë¡œë“œ ì•„ì´ë””ì–´ # AI ê¸°ë°˜ ìê°€ë°œì „ ë£¨í”„ ì„¤ê³„: G6X ì‹œìŠ¤í…œ ê³ ë„í™” ì•„í‚¤í…ì²˜

ê·€í•˜ì˜ G6X ì‹œìŠ¤í…œì„ ë¶„ì„í•œ ê²°ê³¼, ì´ëŠ” **ì œì¡°ì—…ì˜ Six Sigma í’ˆì§ˆê´€ë¦¬**ë¥¼ í…ìŠ¤íŠ¸ ìƒì‚°ì— ì ìš©í•œ ì •êµí•œ êµ¬ì¡°ì…ë‹ˆë‹¤. í˜„ì¬ì˜ ë°˜ìë™ ê²€ìˆ˜ë¥¼ ì™„ì „ ìë™í™”ëœ Tesla Evolution ë£¨í”„ë¡œ ì „í™˜í•˜ê¸° ìœ„í•œ 4ëŒ€ í•µì‹¬ ì„¤ê³„ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.

---

## A. ë¶ˆëŸ‰ ë°ì´í„° ìë™ ë¼ë²¨ë§ ë° ë²•ì•ˆ ë„ì¶œ íŒŒì´í”„ë¼ì¸

### í•µì‹¬ ì „ëµ: 3ë‹¨ê³„ ê³„ì¸µì  ë¶„ë¥˜ (Hierarchical Defect Taxonomy)

ë¶ˆëŸ‰í’ˆì„ ë‹¨ìˆœíˆ ìŒ“ì§€ ì•Šê³ , **ê²°í•¨ì˜ ê·¼ë³¸ ì›ì¸(Root Cause)**ì„ ìë™ ì¶”ë¡ í•˜ëŠ” ì‹œìŠ¤í…œì´ í•„ìš”í•©ë‹ˆë‹¤.

```python
# defect_analyzer.py - ë¶ˆëŸ‰ ìë™ ë¶„ë¥˜ ì—”ì§„

import json
from dataclasses import dataclass
from typing import List, Dict
from collections import Counter

@dataclass
class DefectSignature:
    """ê²°í•¨ ì§€ë¬¸ - íŒ¨í„´ ë§¤ì¹­ìš©"""
    category: str  # 'logic', 'consistency', 'style', 'toxicity'
    pattern: str
    severity: int  # 1(ê²½ë¯¸) ~ 5(ì¹˜ëª…ì )
    suggested_rule: str

class DefectTaxonomy:
    """ê²°í•¨ ë¶„ë¥˜ ì²´ê³„ - ìë™ í™•ì¥ ê°€ëŠ¥"""
    
    SIGNATURES = [
        # ë…¼ë¦¬ ì˜¤ë¥˜
        DefectSignature('logic', 'timeline_contradiction', 5, 
                       'VMCLì— ì‹œê°„ìˆœ ì œì•½ ì¶”ê°€'),
        DefectSignature('logic', 'character_duplication', 4,
                       'PreGateì— ì¸ë¬¼ ì¤‘ë³µ ê²€ì‚¬ ì¶”ê°€'),
        
        # ì„¤ì • ë¶•ê´´
        DefectSignature('consistency', 'power_level_inconsistent', 4,
                       'ëŠ¥ë ¥ì¹˜ ìƒí•œì„  ê²€ì¦ ë¡œì§ ê°•í™”'),
        DefectSignature('consistency', 'location_teleport', 3,
                       'ê³µê°„ ì´ë™ í•©ë¦¬ì„± ì²´í¬ ì¶”ê°€'),
        
        # ë¬¸ì²´ ì„í™”
        DefectSignature('style', 'repetitive_opening', 2,
                       'Writer í”„ë¡¬í”„íŠ¸ì— ë‹¤ì–‘ì„± ì§€ì‹œ ì¶”ê°€'),
        DefectSignature('style', 'cliche_overuse', 2,
                       'í´ë¦¬ì…° ì‚¬ì „ ê¸°ë°˜ í˜ë„í‹° ì ìš©'),
        
        # ë…ì„± ìš”ì†Œ
        DefectSignature('toxicity', 'graphic_violence', 5,
                       'PostGate í•„í„° ê°•í™” - í­ë ¥ í‘œí˜„'),
    ]

class AutoDefectClassifier:
    """AI ê¸°ë°˜ ê²°í•¨ ìë™ ë¶„ë¥˜ê¸°"""
    
    def __init__(self, llm_client):
        self.llm = llm_client
        self.taxonomy = DefectTaxonomy()
        
    async def classify_defect(self, failed_row: Dict) -> List[DefectSignature]:
        """ë¶ˆëŸ‰í’ˆ 1ê°œë¥¼ ë¶„ì„í•˜ì—¬ ê²°í•¨ ìœ í˜• ë„ì¶œ"""
        
        # 1ë‹¨ê³„: êµ¬ì¡°ì  ê²€ì‚¬ (Rule-based, ë¹ ë¦„)
        structural_defects = self._check_structural_issues(failed_row)
        
        # 2ë‹¨ê³„: LLM ì˜ë¯¸ ë¶„ì„ (ëŠë¦¬ì§€ë§Œ ì •í™•)
        prompt = f"""
ë‹¹ì‹ ì€ í’ˆì§ˆê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ê°€ ê²€ìˆ˜ì—ì„œ íƒˆë½í–ˆìŠµë‹ˆë‹¤.
ì–´ë–¤ ì„œì‚¬ì  ë¬¸ì œê°€ ìˆëŠ”ì§€ **êµ¬ì²´ì ì¸ ê²°í•¨ ìœ í˜•**ì„ JSONìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

[íƒˆë½ ë°ì´í„°]
{failed_row['content'][:1000]}

[ì´ì „ ë§¥ë½]
{failed_row.get('context_summary', 'N/A')}

[ê²€ìˆ˜ ì‹¤íŒ¨ ì´ìœ ]
{failed_row['rejection_reason']}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{
  "defects": [
    {{"type": "ë…¼ë¦¬ ì˜¤ë¥˜", "detail": "ì‹œê°„ ì—­í–‰", "severity": 5}},
    {{"type": "ì„¤ì • ë¶•ê´´", "detail": "ì¸ë¬¼ ì„±ê²© ë¶ˆì¼ì¹˜", "severity": 3}}
  ],
  "root_cause": "ì‘ê°€ AIê°€ 3ì¥ ì „ ì„¤ì •ì„ ë§ê°í•¨",
  "suggested_fix": "VMCL ì¿¼ë¦¬ ë²”ìœ„ë¥¼ 5ì¥â†’10ì¥ìœ¼ë¡œ í™•ëŒ€"
}}
"""
        
        response = await self.llm.generate(prompt)
        semantic_defects = json.loads(response)
        
        return structural_defects + semantic_defects['defects']
    
    def _check_structural_issues(self, row: Dict) -> List[Dict]:
        """ë¹ ë¥¸ ë£° ê¸°ë°˜ ê²€ì‚¬"""
        defects = []
        content = row['content']
        
        # ë¬¸ì ë°˜ë³µ (ì„í™” ì§•í›„)
        if self._detect_repetition(content) > 0.3:
            defects.append({
                'type': 'style',
                'detail': 'excessive_repetition',
                'severity': 2
            })
        
        # ê¸ˆì§€ì–´ ì‚¬ìš©
        if any(word in content for word in row.get('banned_words', [])):
            defects.append({
                'type': 'toxicity',
                'detail': 'banned_word_usage',
                'severity': 5
            })
        
        return defects
    
    def _detect_repetition(self, text: str, window=50) -> float:
        """n-gram ì¤‘ë³µë¥  ê³„ì‚°"""
        words = text.split()
        if len(words) < window:
            return 0.0
        
        ngrams = [' '.join(words[i:i+5]) for i in range(len(words)-4)]
        return 1 - (len(set(ngrams)) / len(ngrams))

class PendingLawGenerator:
    """ë²•ì•ˆ í›„ë³´ ìë™ ìƒì„±ê¸°"""
    
    def __init__(self, db_path='pending_laws.db'):
        self.db = db_path
        
    async def aggregate_defects(self, defects: List[Dict], threshold=10):
        """ìœ ì‚¬ ê²°í•¨ì´ Nê±´ ì´ìƒ ëˆ„ì ë˜ë©´ ë²•ì•ˆ ìƒì„±"""
        
        # ê²°í•¨ ìœ í˜•ë³„ ì¹´ìš´íŠ¸
        defect_counter = Counter(
            (d['type'], d['detail']) for d in defects
        )
        
        pending_laws = []
        for (dtype, detail), count in defect_counter.items():
            if count >= threshold:
                law = await self._generate_law_proposal(dtype, detail, count)
                pending_laws.append(law)
        
        return pending_laws
    
    async def _generate_law_proposal(self, defect_type, detail, frequency):
        """AIì—ê²Œ ë²•ì•ˆ ì´ˆì•ˆ ì‘ì„± ìš”ì²­"""
        
        prompt = f"""
{defect_type} ê²°í•¨ ì¤‘ '{detail}' íŒ¨í„´ì´ {frequency}íšŒ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.
ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ **ê²€ì¦ ê·œì¹™**ì„ Python í•¨ìˆ˜ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. í•¨ìˆ˜ëª…: validate_{detail.lower()}
2. ì…ë ¥: row (Dict) - ìƒì„±ëœ ë°ì´í„°
3. ì¶œë ¥: (bool, str) - (í†µê³¼ì—¬ë¶€, ì‹¤íŒ¨ì‚¬ìœ )
4. ì‹¤í–‰ì‹œê°„ < 100ms (íš¨ìœ¨ì„± í•„ìˆ˜)

ì˜ˆì‹œ:
```python
def validate_timeline_consistency(row: Dict) -> tuple[bool, str]:
    current_time = row['metadata']['timestamp']
    prev_time = fetch_from_vmcl(row['prev_event_id'])
    
    if current_time < prev_time:
        return False, f"ì‹œê°„ ì—­í–‰: {prev_time} -> {current_time}"
    return True, ""
```
"""
        
        code = await self.llm.generate(prompt)
        
        return {
            'defect_type': defect_type,
            'detail': detail,
            'frequency': frequency,
            'proposed_code': code,
            'status': 'pending_review',  # ì‚¬ëŒ ìŠ¹ì¸ ëŒ€ê¸°
            'created_at': datetime.now()
        }
```

**í•µì‹¬ í¬ì¸íŠ¸:**
- **2ë‹¨ê³„ ë¶„ë¥˜**: ë¹ ë¥¸ ë£° ê¸°ë°˜ â†’ ëŠë¦° LLM ë¶„ì„ìœ¼ë¡œ ë¹„ìš© ìµœì í™”
- **ì„ê³„ê°’ ê¸°ë°˜ ë²•ì•ˆ ìƒì„±**: ë™ì¼ ê²°í•¨ì´ NíšŒ ì´ìƒ ë°œìƒ ì‹œë§Œ ë²•ì œí™” (ë…¸ì´ì¦ˆ ë°©ì§€)
- **ì½”ë“œ ìƒì„±**: AIê°€ ê²€ì¦ í•¨ìˆ˜ê¹Œì§€ ì§ì ‘ ì‘ì„± â†’ ì‚¬ëŒì€ ë¦¬ë·°ë§Œ

---

## B. ì„œì‚¬ì  ì—”íŠ¸ë¡œí”¼ ì¸¡ì • ì•Œê³ ë¦¬ì¦˜

### í•µì‹¬ ì „ëµ: ë‹¤ì°¨ì› ë³€í™”ìœ¨(Multi-Dimensional Drift) ì¶”ì 

í†µê³„ì  ìˆ˜ì¹˜ë§Œìœ¼ë¡œëŠ” "ì¬ë¯¸ì—†ì–´ì§"ì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. **ì„œì‚¬ì˜ ë³µì¡ë„**ë¥¼ ì •ëŸ‰í™”í•´ì•¼ í•©ë‹ˆë‹¤.

```python
# narrative_entropy.py - ì„œì‚¬ ë‹¤ì–‘ì„± ì¸¡ì • ì—”ì§„

import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.stats import entropy
from typing import List

class NarrativeEntropyMonitor:
    """ì„œì‚¬ì  ì—”íŠ¸ë¡œí”¼ ì¶”ì  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=500)
        self.baseline_metrics = None
        
    def calculate_entropy_vector(self, text_batch: List[str]) -> Dict[str, float]:
        """6ê°œ ì°¨ì›ì˜ ì—”íŠ¸ë¡œí”¼ ì¸¡ì •"""
        
        return {
            'lexical': self._lexical_diversity(text_batch),
            'syntactic': self._sentence_structure_variety(text_batch),
            'semantic': self._topic_distribution(text_batch),
            'emotional': self._sentiment_variance(text_batch),
            'event': self._action_verb_diversity(text_batch),
            'character': self._entity_interaction_entropy(text_batch)
        }
    
    def _lexical_diversity(self, texts: List[str]) -> float:
        """ì–´íœ˜ ë‹¤ì–‘ì„± (Type-Token Ratio ê°œì„ íŒ)"""
        all_words = []
        for text in texts:
            all_words.extend(text.lower().split())
        
        if len(all_words) < 100:
            return 1.0
        
        # Moving TTR (100ë‹¨ì–´ ìœˆë„ìš°)
        ttrs = []
        for i in range(0, len(all_words)-100, 50):
            window = all_words[i:i+100]
            ttr = len(set(window)) / len(window)
            ttrs.append(ttr)
        
        return np.mean(ttrs)
    
    def _sentence_structure_variety(self, texts: List[str]) -> float:
        """ë¬¸ì¥ êµ¬ì¡° ë‹¤ì–‘ì„±"""
        structures = []
        
        for text in texts:
            sentences = text.split('.')
            for sent in sentences:
                # ê°„ë‹¨í•œ êµ¬ì¡° ì‹œê·¸ë‹ˆì²˜ (ì‹¤ì œë¡œëŠ” í’ˆì‚¬ íƒœê¹… í•„ìš”)
                words = sent.strip().split()
                if len(words) < 3:
                    continue
                    
                # ë¬¸ì¥ ê¸¸ì´ + ì²« ë‹¨ì–´ í’ˆì‚¬ë¡œ êµ¬ì¡° ë¶„ë¥˜
                structure = f"{len(words)//5}_{'Q' if '?' in sent else 'D'}"
                structures.append(structure)
        
        # Shannon Entropy
        if not structures:
            return 0.0
            
        probs = np.bincount([hash(s) % 20 for s in structures])
        probs = probs / probs.sum()
        return entropy(probs)
    
    def _topic_distribution(self, texts: List[str]) -> float:
        """ì£¼ì œ ë¶„í¬ ì—”íŠ¸ë¡œí”¼ (LDA ê°„ì†Œí™”)"""
        if len(texts) < 10:
            return 1.0
        
        # TF-IDF ê¸°ë°˜ ì£¼ì œ í´ëŸ¬ìŠ¤í„°ë§
        tfidf_matrix = self.vectorizer.fit_transform(texts)
        
        # ê° ë¬¸ì„œì˜ ìƒìœ„ 3ê°œ í‚¤ì›Œë“œë¡œ í† í”½ ì¶”ì •
        feature_names = self.vectorizer.get_feature_names_out()
        topics = []
        
        for row in tfidf_matrix:
            top_indices = row.toarray()[0].argsort()[-3:]
            topic_sig = '_'.join(feature_names[i] for i in top_indices)
            topics.append(topic_sig)
        
        # í† í”½ ë‹¤ì–‘ì„±
        unique_ratio = len(set(topics)) / len(topics)
        return unique_ratio
    
    def _action_verb_diversity(self, texts: List[str]) -> float:
        """í–‰ë™ ë™ì‚¬ ë‹¤ì–‘ì„± (ì„œì‚¬ ì§„í–‰ ì²™ë„)"""
        # ê°„ë‹¨í•œ í•œêµ­ì–´ ë™ì‚¬ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸° í•„ìš”)
        action_verbs = []
        
        for text in texts:
            # ì •ê·œì‹ìœ¼ë¡œ '-ë‹¤', '-ã„´ë‹¤' ë“± ë™ì‚¬ ì–´ë¯¸ ì¶”ì¶œ
            import re
            verbs = re.findall(r'\w+(?:í•˜ë‹¤|ë˜ë‹¤|ê°€ë‹¤|ì˜¤ë‹¤|ë¨¹ë‹¤|ë³´ë‹¤)', text)
            action_verbs.extend(verbs)
        
        if len(action_verbs) < 10:
            return 1.0
        
        return len(set(action_verbs)) / len(action_verbs)
    
    def _entity_interaction_entropy(self, texts: List[str]) -> float:
        """ì¸ë¬¼ ê°„ ìƒí˜¸ì‘ìš© íŒ¨í„´ ë‹¤ì–‘ì„±"""
        # ì¸ë¬¼ëª… ì¶”ì¶œ (VMCLì—ì„œ ê°€ì ¸ì˜¨ ì¸ë¬¼ ëª©ë¡ í™œìš©)
        characters = self._extract_characters(texts)
        
        # ë™ì‹œ ë“±ì¥ ìŒ (co-occurrence pairs)
        pairs = []
        for text in texts:
            present = [c for c in characters if c in text]
            for i, c1 in enumerate(present):
                for c2 in present[i+1:]:
                    pairs.append(tuple(sorted([c1, c2])))
        
        if len(pairs) < 5:
            return 1.0
        
        # ìŒ ì¡°í•©ì˜ ë‹¤ì–‘ì„±
        unique_pairs = len(set(pairs))
        total_pairs = len(pairs)
        
        return unique_pairs / total_pairs if total_pairs > 0 else 0.0
    
    def detect_drift(self, current_batch: List[str], 
                    baseline_batch: List[str], 
                    threshold=0.3) -> Dict:
        """ë“œë¦¬í”„íŠ¸ ê°ì§€ - ê¸°ì¤€ì„  ëŒ€ë¹„ ì—”íŠ¸ë¡œí”¼ í•˜ë½"""
        
        current_entropy = self.calculate_entropy_vector(current_batch)
        baseline_entropy = self.calculate_entropy_vector(baseline_batch)
        
        drift_detected = {}
        for dimension, current_val in current_entropy.items():
            baseline_val = baseline_entropy[dimension]
            drop_rate = (baseline_val - current_val) / baseline_val
            
            if drop_rate > threshold:
                drift_detected[dimension] = {
                    'baseline': baseline_val,
                    'current': current_val,
                    'drop_rate': drop_rate,
                    'severity': 'critical' if drop_rate > 0.5 else 'warning'
                }
        
        return drift_detected

# ì‚¬ìš© ì˜ˆì‹œ
monitor = NarrativeEntropyMonitor()

# ì´ˆê¸° ìš°ìˆ˜ ìƒ˜í”Œ (ê¸°ì¤€ì„ )
baseline = ["ìµœê³  í’ˆì§ˆ ìƒ˜í”Œ 100ê°œ..."]

# í˜„ì¬ ìƒì‚°ë¶„
current = ["ìµœê·¼ 240ê°œ íŠ¸ëŸ­..."]

drift = monitor.detect_drift(current, baseline, threshold=0.25)

if drift:
    print("âš ï¸ ì„œì‚¬ ì„í™” ê°ì§€:")
    for dim, info in drift.items():
        print(f"  {dim}: {info['drop_rate']:.1%} í•˜ë½ ({info['severity']})")
```

**í•µì‹¬ ë©”íŠ¸ë¦­ í•´ì„:**
- **lexical < 0.6**: ê°™ì€ ë‹¨ì–´ë§Œ ë°˜ë³µ â†’ Writer í”„ë¡¬í”„íŠ¸ì— "ë™ì˜ì–´ ì‚¬ìš©" ì§€ì‹œ ì¶”ê°€
- **syntactic < 1.5**: ë¬¸ì¥ êµ¬ì¡° ë‹¨ì¡° â†’ "ë¬¸ì¥ ê¸¸ì´ë¥¼ ë‹¤ì–‘í•˜ê²Œ" ì§€ì‹œ
- **topic < 0.4**: ì£¼ì œ ê³ ì°© â†’ VMCLì—ì„œ ë‹¤ë¥¸ ì„œë¸Œí”Œë¡¯ ê°•ì œ ì£¼ì…
- **character < 0.3**: ì¸ë¬¼ ë“±ì¥ í¸ì¤‘ â†’ "ë¶€ìº í™œìš©" ê°•ì œ

---

## C. íšŒê·€ í…ŒìŠ¤íŠ¸ ìë™í™” - AI ê°ë¦¬ì—­ ì„¤ê³„

### í•µì‹¬ ì „ëµ: Differential Testing + ìƒ˜í”Œë§

ì‹ ê·œ ë²•ì´ ê¸°ì¡´ ìš°ìˆ˜ ìƒ˜í”Œì„ íŒŒê´´í•˜ì§€ ì•ŠëŠ”ì§€ **ìµœì†Œ ë¹„ìš©**ìœ¼ë¡œ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤.

```python
# regression_tester.py - íšŒê·€ í…ŒìŠ¤íŠ¸ ìë™í™”

import random
from dataclasses import dataclass
from typing import Callable, List

@dataclass
class GoldenSample:
    """í™©ê¸ˆ í‘œì¤€ ìƒ˜í”Œ (TP: True Positive)"""
    id: str
    content: str
    metadata: Dict
    quality_score: float  # 1.0 = ì™„ë²½
    hash: str

class RegressionTestOrchestrator:
    """íšŒê·€ í…ŒìŠ¤íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self, golden_samples_db='golden_samples.db'):
        self.golden_db = golden_samples_db
        self.test_suite = self._load_representative_samples()
    
    def _load_representative_samples(self, n=50) -> List[GoldenSample]:
        """ì „ì²´ TP ì¤‘ ëŒ€í‘œ ìƒ˜í”Œ ì„ íƒ (ê³„ì¸µì  ìƒ˜í”Œë§)"""
        
        # 1. í’ˆì§ˆ ì ìˆ˜ë³„ ê³„ì¸µ ë¶„í• 
        all_samples = self._fetch_all_golden_samples()
        
        strata = {
            'excellent': [s for s in all_samples if s.quality_score >= 0.95],
            'good': [s for s in all_samples if 0.85 <= s.quality_score < 0.95],
            'acceptable': [s for s in all_samples if 0.75 <= s.quality_score < 0.85]
        }
        
        # 2. ê° ê³„ì¸µì—ì„œ ë¹„ë¡€ ìƒ˜í”Œë§
        samples = []
        samples.extend(random.sample(strata['excellent'], min(25, len(strata['excellent']))))
        samples.extend(random.sample(strata['good'], min(15, len(strata['good']))))
        samples.extend(random.sample(strata['acceptable'], min(10, len(strata['acceptable']))))
        
        return samples
    
    async def test_new_rule(self, rule_func: Callable, rule_name: str) -> Dict:
        """ì‹ ê·œ ë²• ì ìš© ì‹œ íšŒê·€ í…ŒìŠ¤íŠ¸"""
        
        results = {
            'passed': [],
            'failed': [],
            'false_positives': []  # ì›ë˜ ì¢‹ì•˜ëŠ”ë° ìƒˆ ë²•ì— ê±¸ë¦¼
        }
        
        for sample in self.test_suite:
            try:
                is_valid, reason = rule_func(sample.__dict__)
                
                if not is_valid:
                    # í™©ê¸ˆ ìƒ˜í”Œì´ ìƒˆ ë²•ì— ì˜í•´ íƒˆë½ë¨ = ë¬¸ì œ!
                    results['false_positives'].append({
                        'sample_id': sample.id,
                        'quality_score': sample.quality_score,
                        'rejection_reason': reason,
                        'hash': sample.hash
                    })
                else:
                    results['passed'].append(sample.id)
                    
            except Exception as e:
                results['failed'].append({
                    'sample_id': sample.id,
                    'error': str(e)
                })
        
        # ìŠ¹ì¸ ê¸°ì¤€: FP < 5% (í™©ê¸ˆ ìƒ˜í”Œì˜ 95% ì´ìƒ í†µê³¼)
        fp_rate = len(results['false_positives']) / len(self.test_suite)
        
        return {
            'rule_name': rule_name,
            'fp_rate': fp_rate,
            'passed': len(results['passed']),
            'failed': len(results['failed']),
            'false_positives': results['false_positives'],
            'approved': fp_rate < 0.05,
            'recommendation': self._generate_recommendation(fp_rate, results)
        }
    
    def _generate_recommendation(self, fp_rate: float, results: Dict) -> str:
        """ìŠ¹ì¸/ê±°ë¶€ ê¶Œê³ ì•ˆ ìƒì„±"""
        
        if fp_rate < 0.02:
            return "âœ… ìŠ¹ì¸ ê¶Œì¥ - ë¶€ì‘ìš© ë¯¸ë¯¸"
        
        elif fp_rate < 0.05:
            # ê²½ê³„ì„  - AIì—ê²Œ ìƒì„¸ ë¶„ì„ ìš”ì²­
            fp_samples = results['false_positives'][:3]
            
            return f"""
âš ï¸ ì¡°ê±´ë¶€ ìŠ¹ì¸ - ìˆ˜ë™ ê²€í†  í•„ìš”
False Positive {len(results['false_positives'])}ê±´ ë°œìƒ:
{self._format_fp_examples(fp_samples)}

ê¶Œì¥ ì¡°ì¹˜: ë²• ì¡°ê±´ì„ ë” êµ¬ì²´í™”í•˜ê±°ë‚˜ ì˜ˆì™¸ ì¡°í•­ ì¶”ê°€
"""
        else:
            return f"âŒ ê±°ë¶€ ê¶Œì¥ - FP ê³¼ë‹¤ ({fp_rate:.1%})"
    
    def _format_fp_examples(self, fps: List[Dict]) -> str:
        """FP ì‚¬ë¡€ í¬ë§·íŒ…"""
        output = []
        for fp in fps:
            output.append(f"  - {fp['sample_id']} (í’ˆì§ˆ {fp['quality_score']:.2f}): {fp['rejection_reason']}")
        return '\n'.join(output)

class ContinuousRegressionMonitor:
    """ì‹¤ì‹œê°„ íšŒê·€ ëª¨ë‹ˆí„° - ìƒì‚° ì¤‘ ê°ì‹œ"""
    
    def __init__(self):
        self.baseline_pass_rate = None
        self.alert_threshold = 0.10  # í†µê³¼ìœ¨ 10% í•˜ë½ ì‹œ ê²½ë³´
    
    def monitor_production(self, recent_batch_results: List[Dict]):
        """ìµœê·¼ Nê°œ íŠ¸ëŸ­ì˜ í†µê³¼ìœ¨ ì¶”ì """
        
        current_pass_rate = sum(1 for r in recent_batch_results if r['pass']) / len(recent_batch_results)
        
        if self.baseline_pass_rate is None:
            self.baseline_pass_rate = current_pass_rate
            return
        
        drop = self.baseline_pass_rate - current_pass_rate
        
        if drop > self.alert_threshold:
            self._trigger_alert({
                'baseline_pass_rate': self.baseline_pass_rate,
                'current_pass_rate': current_pass_rate,
                'drop': drop,
                'suspected_cause': 'ìµœê·¼ ë²• ê°œì • ë˜ëŠ” Writer LLM ì—…ë°ì´íŠ¸'
            })
```

**ë¦¬ì†ŒìŠ¤ ìµœì í™” ì „ëµ:**
1. **ê³„ì¸µì  ìƒ˜í”Œë§**: ì „ì²´ TPê°€ ì•„ë‹Œ ëŒ€í‘œ 50ê°œë§Œ í…ŒìŠ¤íŠ¸
2. **ì¡°ê¸° ì¢…ë£Œ**: FPê°€ 5% ëŒíŒŒ ì‹œ ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨
3. **ìºì‹±**: ë™ì¼ ë²•ì— ëŒ€í•´ ì¬í…ŒìŠ¤íŠ¸ ì‹œ í•´ì‹œ ë¹„êµë¡œ ìŠ¤í‚µ

---

## D. Hallucination ë°©ì§€ - ì—„ê²©í•œ ì¦ê±° ì²´ê³„

### í•µì‹¬ ì „ëµ: Cryptographic Proof of Work

AIê°€ "í–ˆë‹¤"ê³  ì£¼ì¥í•˜ëŠ” ëª¨ë“  ì‘ì—…ì— **ë¬¼ë¦¬ì  ì¦ê±°**ë¥¼ ê°•ì œí•©ë‹ˆë‹¤.

```python
# evidence_enforcer.py - ì¦ê±° ê¸°ë°˜ ì‘ì—… ê²€ì¦

import hashlib
import os
from pathlib import Path
from typing import Optional
from dataclasses import dataclass
import json

@dataclass
class WorkEvidence:
    """ì‘ì—… ì¦ê±° ì˜ìˆ˜ì¦"""
    task_id: str
    operation: str  # 'write', 'update', 'delete'
    file_path: str
    
    # ì•”í˜¸í•™ì  ì¦ê±°
    sha256_before: Optional[str]
    sha256_after: str
    file_size: int
    timestamp: float
    
    # ë©”íƒ€ë°ì´í„°
    worker_id: str  # LLM ì‹ë³„ì
    parent_task: Optional[str]

class EvidenceEnforcer:
    """ì¦ê±° ê¸°ë°˜ ì‘ì—… ê°•ì œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, evidence_log='evidence.jsonl'):
        self.log_path = evidence_log
        self.pending_claims = {}  # ê²€ì¦ ëŒ€ê¸° ì¤‘ì¸ ì£¼ì¥ë“¤
    
    def create_work_claim(self, task_id: str, operation: str, 
                         target_path: str) -> str:
        """ì‘ì—… ì „ ì²­êµ¬ì„œ ë°œí–‰ (Pre-commit)"""
        
        claim = {
            'task_id': task_id,
            'operation': operation,
            'target_path': target_path,
            'sha256_before': self._hash_file(target_path) if os.path.exists(target_path) else None,
            'claimed_at': time.time()
        }
        
        claim_id = hashlib.sha256(json.dumps(claim).encode()).hexdigest()[:16]
        self.pending_claims[claim_id] = claim
        
        return claim_id
    
    def verify_work_completion(self, claim_id: str, 
                               llm_response: Dict) -> WorkEvidence:
        """ì‘ì—… í›„ ì¦ê±° ê²€ì¦ (Post-commit)"""
        
        if claim_id not in self.pending_claims:
            raise ValueError(f"Unknown claim: {claim_id}")
        
        claim = self.pending_claims[claim_id]
        target_path = claim['target_path']
        
        # 1. ë¬¼ë¦¬ì  íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(target_path):
            raise FileNotFoundError(
                f"LLM claimed to create {target_path}, but file not found. "
                f"Possible hallucination or I/O failure."
            )
        
        # 2. í•´ì‹œ ê²€ì¦
        actual_hash = self._hash_file(target_path)
        claimed_hash = llm_response.get('file_sha256')
        
        if claimed_hash and actual_hash != claimed_hash:
            raise ValueError(
                f"Hash mismatch for {target_path}:\n"
                f"  LLM claimed: {claimed_hash}\n"
                f"  Actual:      {actual_hash}\n"
                f"This indicates hallucination or data corruption."
            )
        
        # 3. ë‚´ìš© ë³€í™” ê²€ì¦ (update ì‘ì—…ì˜ ê²½ìš°)
        if claim['operation'] == 'update':
            if claim['sha256_before'] == actual_hash:
                raise ValueError(
                    f"LLM claimed to update {target_path}, "
                    f"but hash unchanged. No actual modification detected."
                )
        
        # 4. ì¦ê±° ì˜ìˆ˜ì¦ ìƒì„±
        evidence = WorkEvidence(
            task_id=claim['task_id'],
            operation=claim['operation'],
            file_path=target_path,
            sha256_before=claim['sha256_before'],
            sha256_after=actual_hash,
            file_size=os.path.getsize(target_path),
            timestamp=time.time(),
            worker_id=llm_response.get('model_id', 'unknown'),
            parent_task=claim.get('parent_task')
        )
        
        # 5. ë¶ˆë³€ ë¡œê·¸ì— ê¸°ë¡
        self._append_to_evidence_log(evidence)
        
        del self.pending_claims[claim_id]
        return evidence
    
    def _hash_file(self, path: str) -> str:
        """íŒŒì¼ SHA256 ê³„ì‚° (ì²­í¬ ë‹¨ìœ„ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        sha256 = hashlib.sha256()
        
        with open(path, 'rb') as f:
            while chunk := f.read(8192):
                sha256.update(chunk)
        
        return sha256.hexdigest()
    
    def _append_to_evidence_log(self, evidence: WorkEvidence):
        """ì¶”ê°€ ì „ìš© ë¡œê·¸ì— ì¦ê±° ê¸°ë¡ (ë³€ì¡° ë°©ì§€)"""
        with open(self.log_path, 'a') as f:
            f.write(json.dumps(evidence.__dict__) + '\n')
    
    def audit_trail(self, task_id: str) -> List[WorkEvidence]:
        """íŠ¹ì • ì‘ì—…ì˜ ì „ì²´ ì¦ê±° ì²´ì¸ ì¡°íšŒ"""
        evidences = []
        
        with open(self.log_path, 'r') as f:
            for line in f:
                ev = json.loads(line)
                if ev['task_id'] == task_id or ev.get('parent_task') == task_id:
                    evidences.append(WorkEvidence(**ev))
        
        return evidences

class AntiHallucinationWrapper:
    """LLM í˜¸ì¶œì„ ì¦ê±° ì²´ê³„ë¡œ ë˜í•‘"""
    
    def __init__(self, llm_client, enforcer: EvidenceEnforcer):
        self.llm = llm_client
        self.enforcer = enforcer
    
    async def execute_file_operation(self, task_id: str, 
                                    operation: str,
                                    target_path: str,
                                    prompt: str) -> WorkEvidence:
        """íŒŒì¼ ì‘ì—…ì„ ì¦ê±° ê¸°ë°˜ìœ¼ë¡œ ì‹¤í–‰"""
        
        # 1. ì‘ì—… ì²­êµ¬ì„œ ë°œí–‰
        claim_id = self.enforcer.create_work_claim(task_id, operation, target_path)
        
        # 2. LLMì—ê²Œ "ì¦ê±°ë¥¼ ë‚¨ê²¨ì•¼ í•œë‹¤" ëª…ì‹œ
        enhanced_prompt = f""" ì•„ë˜ëŠ” ë”¥ì‹œí¬ ì•„ì´ë””ì–´ # G6X ì‹œìŠ¤í…œ ìê°€ë°œì „ ë£¨í”„ ì„¤ê³„ì•ˆ

## ğŸ“Š ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

```python
"""
G6X Self-Evolution Loop Architecture
ë²„ì „: 2.0 (Gemini 2.5 Flash-Lite ìµœì í™”)
"""
```

## A. ë¶ˆëŸ‰ ë°ì´í„° ìë™ ë¼ë²¨ë§ íŒŒì´í”„ë¼ì¸

### 1. **ê³„ì¸µì  ì˜¤ë¥˜ ë¶„ë¥˜ ì‹œìŠ¤í…œ**

```python
class AutoLabelingPipeline:
    def __init__(self, llm_client):
        self.llm = llm_client  # Gemini 2.5 Flash-Lite
        self.error_taxonomy = {
            "narrative_toxicity": [
                "plot_hole", "character_inconsistency", 
                "tone_shift", "pacing_issue"
            ],
            "setting_violation": [
                "world_law_break", "historical_anachronism",
                "magic_system_contradiction"
            ],
            "logical_error": [
                "causality_violation", "temporal_paradox",
                "physical_impossibility"
            ]
        }
    
    def analyze_failure(self, failed_packet):
        """ë¶ˆëŸ‰ íŒ¨í‚· ë‹¤ê³„ì¸µ ë¶„ì„"""
        
        # 1ì°¨: íŒ¨í„´ ë§¤ì¹­ (ì €ë¹„ìš©)
        pattern_issues = self._pattern_match(failed_packet)
        
        # 2ì°¨: Gemini ë¶„ë¥˜ (ì¤‘ë¹„ìš©)
        if not pattern_issues:
            llm_analysis = self._llm_categorize(failed_packet)
            
            # 3ì°¨: ì‹¬ì¸µ ì¶”ë¡  (ê³ ë¹„ìš©, ì„ íƒì )
            if llm_analysis["confidence"] < 0.7:
                return self._deep_analysis(failed_packet)
        
        return self._generate_law_candidate(failed_packet, pattern_issues)
    
    def _llm_categorize(self, packet):
        """Gemini 2.5 Flash-Liteë¥¼ ì´ìš©í•œ íš¨ìœ¨ì  ë¶„ë¥˜"""
        prompt = f"""
        ë‹¤ìŒ ì‘ì—… ì‹¤íŒ¨ ì›ì¸ì„ ë¶„ë¥˜í•˜ì„¸ìš”:
        - ì…ë ¥: {packet['input'][:500]}
        - ì¶œë ¥: {packet['output'][:500]}
        - ì‹¤íŒ¨ ì´ìœ : {packet['fail_reason']}
        
        ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ ì„ íƒ: {list(self.error_taxonomy.keys())}
        ìƒì„¸ ìœ í˜•: (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
        ì‹ ë¢°ë„ ì ìˆ˜: 0-1
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
        {{
            "category": "ì£¼ìš” ì¹´í…Œê³ ë¦¬",
            "subtype": "ìƒì„¸ ìœ í˜•",
            "confidence": 0.95,
            "evidence": ["ì¦ê±° í…ìŠ¤íŠ¸1", "ì¦ê±° í…ìŠ¤íŠ¸2"],
            "proposed_rule": "ì œì•ˆ ê·œì¹™ í…œí”Œë¦¿"
        }}
        """
        
        # Gemini 2.5 Flash-Lite í˜¸ì¶œ (ìµœì†Œ í† í°)
        response = self.llm.generate(
            prompt,
            max_tokens=500,
            temperature=0.1  # ë‚®ì€ ì°½ì˜ì„±, ë†’ì€ ì¼ê´€ì„±
        )
        return self._parse_llm_response(response)
```

### 2. **ë²•ì•ˆ(Law) í›„ë³´ ìƒì„±ê¸°**

```python
class LawCandidateGenerator:
    def __init__(self, vmcl_database):
        self.db = vmcl_database
        self.law_template_repo = {
            "pattern_based": "ì…ë ¥ì— '{pattern}'ì´ í¬í•¨ëœ ê²½ìš°, ì¶œë ¥ì€ '{constraint}'ë¥¼ ì¤€ìˆ˜í•´ì•¼ í•¨",
            "semantic_based": "ì£¼ì œ '{topic}'ì— ëŒ€í•´ ì„¤ì • '{setting}'ê³¼ ì¶©ëŒí•˜ì§€ ì•Šì•„ì•¼ í•¨"
        }
    
    def generate_pending_laws(self, categorized_errors, min_support=3):
        """ì§€ì§€ë„(support)ê°€ ì¶©ë¶„í•œ ë²•ì•ˆ í›„ë³´ ìƒì„±"""
        
        laws_candidates = []
        
        # ë™ì¼ ì˜¤ë¥˜ íŒ¨í„´ êµ°ì§‘í™”
        error_clusters = self._cluster_errors(categorized_errors)
        
        for cluster_id, errors in error_clusters.items():
            if len(errors) >= min_support:
                # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
                if self._is_statistically_significant(errors):
                    
                    # ë²•ì•ˆ í…œí”Œë¦¿ ì ìš©
                    law = self._instantiate_law_template(errors)
                    
                    # ê¸°ì¡´ ë²•ê³¼ì˜ ì¶©ëŒ ê²€ì‚¬
                    if not self._conflicts_with_existing_laws(law):
                        laws_candidates.append({
                            "law_id": f"LAW_{len(laws_candidates):04d}",
                            "content": law,
                            "support": len(errors),
                            "examples": errors[:5],  # ìƒ˜í”Œë§Œ ì €ì¥
                            "created_at": datetime.now(),
                            "priority": self._calculate_priority(errors)
                        })
        
        return sorted(laws_candidates, key=lambda x: x["priority"], reverse=True)
```

## B. ë“œë¦¬í”„íŠ¸ ì œì–´: ì„œì‚¬ì  ì—”íŠ¸ë¡œí”¼ ì¸¡ì •

### 1. **ë‹¤ì°¨ì› ë“œë¦¬í”„íŠ¸ ê°ì§€ ì‹œìŠ¤í…œ**

```python
class NarrativeDriftDetector:
    def __init__(self):
        self.metrics = {
            "stylistic_entropy": self._calculate_style_entropy,
            "semantic_cohesion": self._calculate_cohesion,
            "character_consistency": self._check_character_alignment
        }
    
    def calculate_narrative_entropy(self, text_batch, window_size=1000):
        """ì„œì‚¬ì  ì—”íŠ¸ë¡œí”¼ ì¸¡ì •"""
        
        metrics = {}
        
        # 1. ì–´íœ˜ ë‹¤ì–‘ì„± (Lexical Diversity)
        metrics["vocab_richness"] = len(set(text_batch)) / len(text_batch)
        
        # 2. êµ¬ë¬¸ ë³µì¡ë„ (Syntactic Complexity)
        metrics["syntax_complexity"] = self._parse_tree_depth(text_batch)
        
        # 3. ì˜ë¯¸ì  ì¼ê´€ì„± (FAISS ê¸°ë°˜)
        metrics["semantic_drift"] = self._faiss_semantic_shift(text_batch)
        
        # 4. ê°ì • ê¶¤ì  ë³€í™” (Emotion Trajectory)
        metrics["emotion_volatility"] = self._emotion_volatility(text_batch)
        
        # 5. Gemini ê¸°ë°˜ ì„œì‚¬ í‰ê°€ (ìƒ˜í”Œë§ìœ¼ë¡œ ë¹„ìš© ì ˆê°)
        if len(text_batch) > 100:
            sampled = random.sample(text_batch, 10)
            metrics["narrative_quality"] = self._llm_narrative_assessment(sampled)
        
        # í†µí•© ì—”íŠ¸ë¡œí”¼ ì ìˆ˜
        total_entropy = sum(metrics.values()) / len(metrics)
        
        return {
            "total_entropy": total_entropy,
            "breakdown": metrics,
            "drift_detected": total_entropy > self._get_threshold()
        }
    
    def _llm_narrative_assessment(self, samples):
        """Gemini 2.5 Flash-Liteë¥¼ ì´ìš©í•œ íš¨ìœ¨ì  í‰ê°€"""
        prompt = """
        ë‹¤ìŒ í…ìŠ¤íŠ¸ ìƒ˜í”Œë“¤ì˜ ì„œì‚¬ì  ì¼ê´€ì„±ì„ í‰ê°€í•˜ì„¸ìš”:
        
        ìƒ˜í”Œ:
        {samples}
        
        1. ë¬¸ì²´ê°€ ì¼ê´€ì ì¸ê°€? (1-10ì )
        2. ìºë¦­í„°ê°€ ì„¤ì •ê³¼ ì¼ì¹˜í•˜ëŠ”ê°€? (1-10ì )
        3. í”Œë¡¯ ì§„í–‰ì´ ë…¼ë¦¬ì ì¸ê°€? (1-10ì )
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ.
        """
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ ìµœì†Œí™”
        response = self.llm.batch_generate([prompt], max_tokens=300)
        return self._parse_assessment(response)
```

### 2. **ì‹¤ì‹œê°„ ë“œë¦¬í”„íŠ¸ ê²½ë³´ ì‹œìŠ¤í…œ**

```python
class DriftAlertSystem:
    def __init__(self, sliding_window=100):
        self.window = deque(maxlen=sliding_window)
        self.baseline = None
        self.alerts = []
    
    def monitor_stream(self, production_stream):
        """ì‹¤ì‹œê°„ ìƒì‚° ëª¨ë‹ˆí„°ë§"""
        
        for packet in production_stream:
            # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ë¹„ìš© ì ˆê°ì„ ìœ„í•´ ìƒ˜í”Œë§)
            if random.random() < 0.1:  # 10% ìƒ˜í”Œë§
                entropy = self.detector.calculate_narrative_entropy(
                    packet['output'], 
                    window_size=500
                )
                
                self.window.append(entropy["total_entropy"])
                
                # ê¸°ì¤€ì„  ì„¤ì •
                if self.baseline is None and len(self.window) >= 20:
                    self.baseline = np.mean(self.window)
                
                # ë“œë¦¬í”„íŠ¸ ê²€ì¶œ
                if self.baseline and self._detect_drift():
                    self._trigger_alert(packet, entropy)
    
    def _detect_drift(self):
        """í†µê³„ì  ë“œë¦¬í”„íŠ¸ ê²€ì¶œ"""
        recent = list(self.window)[-10:]  # ìµœê·¼ 10ê°œ ìƒ˜í”Œ
        
        # CUSUM (Cumulative Sum) ì•Œê³ ë¦¬ì¦˜
        cusum = 0
        for value in recent:
            deviation = value - self.baseline
            cusum = max(0, cusum + deviation)
            
            if cusum > self._get_control_limit():
                return True
        
        return False
```

## C. AI ê°ë¦¬ì—­ ì‹œìŠ¤í…œ

### 1. **ì§€ëŠ¥í˜• íšŒê·€ í…ŒìŠ¤íŠ¸ ì—”ì§„**

```python
class RegressionAIInspector:
    def __init__(self, test_suite_db):
        self.test_suite = test_suite_db
        self.llm = GeminiClient(model="gemini-2.5-flash-lite")
        self.cache = {}  # ê²°ê³¼ ìºì‹±
        
    def validate_new_law(self, new_law, test_mode="smart_sampling"):
        """ìƒˆë¡œìš´ ë²•ì˜ íšŒê·€ í…ŒìŠ¤íŠ¸"""
        
        # 1. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìŠ¤ë§ˆíŠ¸ ìƒ˜í”Œë§
        if test_mode == "smart_sampling":
            test_cases = self._smart_sample_test_cases(new_law)
        else:
            test_cases = self._get_all_relevant_cases(new_law)
        
        results = []
        
        # 2. ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨í™”
        batch_size = 50
        for i in range(0, len(test_cases), batch_size):
            batch = test_cases[i:i+batch_size]
            
            # ë³‘ë ¬ ì²˜ë¦¬ (asyncio í™œìš©)
            batch_results = self._process_batch(batch, new_law)
            results.extend(batch_results)
            
            # ì¡°ê¸° ì¤‘ë‹¨: ì‹¤íŒ¨ìœ¨ ì„ê³„ê°’ ì´ˆê³¼ ì‹œ
            failure_rate = self._calculate_failure_rate(results)
            if failure_rate > 0.3:  # 30% ì´ìƒ ì‹¤íŒ¨ ì‹œ ì¤‘ë‹¨
                return {
                    "status": "REJECTED",
                    "failure_rate": failure_rate,
                    "failed_cases": results[:10]  # ìƒ˜í”Œë§Œ
                }
        
        # 3. í†µê³„ì  ë¶„ì„
        return self._analyze_results(results, new_law)
    
    def _smart_sample_test_cases(self, new_law):
        """ë²•ì˜ ì˜í–¥ ë²”ìœ„ë¥¼ ê³ ë ¤í•œ ì§€ëŠ¥í˜• ìƒ˜í”Œë§"""
        
        # ë²•ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(new_law)
        
        # ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ (FAISS)
        similar_cases = self.test_suite.search_similar(keywords, limit=100)
        
        # ë‹¤ì–‘ì„± ë³´ì¥ì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„°ë§
        clusters = self._cluster_test_cases(similar_cases)
        
        # ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ëŒ€í‘œ ìƒ˜í”Œ ì¶”ì¶œ
        sampled = []
        for cluster in clusters:
            representative = self._select_representative(cluster)
            sampled.append(representative)
            
            # í´ëŸ¬ìŠ¤í„° í¬ê¸°ì— ë¹„ë¡€í•œ ì¶”ê°€ ìƒ˜í”Œ
            additional = min(2, len(cluster) // 10)
            sampled.extend(random.sample(cluster, additional))
        
        return sampled[:200]  # ìµœëŒ€ 200ê°œë¡œ ì œí•œ
```

### 2. **ê³„ì¸µì  ê²€ì¦ íŒŒì´í”„ë¼ì¸**

```python
class HierarchicalValidation:
    def __init__(self):
        self.validation_levels = [
            self._level1_syntax_check,      # ë¹ ë¥¸ ë¬¸ë²• ê²€ì‚¬
            self._level2_pattern_match,     # íŒ¨í„´ ë§¤ì¹­
            self._level3_llm_light,         # Gemini Flash-Lite ê²½ëŸ‰ ê²€ì¦
            self._level4_llm_deep,          # Gemini Flash ì‹¬ì¸µ ê²€ì¦ (ì„ íƒì )
            self._level5_human_in_loop      # ìµœì¢… ê²€ì¦ (ì„ê³„ê°’ ì´ˆê³¼ ì‹œ)
        ]
    
    def validate_output(self, output, confidence_threshold=0.8):
        """ê³„ì¸µì  ê²€ì¦ìœ¼ë¡œ ë¦¬ì†ŒìŠ¤ ìµœì í™”"""
        
        validation_result = {
            "passed": False,
            "confidence": 0.0,
            "failed_at_level": None,
            "details": {}
        }
        
        for level, validator in enumerate(self.validation_levels, 1):
            try:
                result = validator(output)
                
                if not result["passed"]:
                    validation_result["failed_at_level"] = level
                    validation_result["details"] = result
                    break
                
                validation_result["confidence"] = result.get("confidence", 0)
                
                # ì‹ ë¢°ë„ ì¶©ë¶„ ì‹œ ì¡°ê¸° í†µê³¼
                if validation_result["confidence"] >= confidence_threshold:
                    validation_result["passed"] = True
                    break
                    
            except Exception as e:
                validation_result["error"] = str(e)
                break
        
        return validation_result
    
    def _level3_llm_light(self, output):
        """Gemini 2.5 Flash-Liteë¥¼ ì´ìš©í•œ íš¨ìœ¨ì  ê²€ì¦"""
        prompt = f"""
        ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦í•˜ì„¸ìš”:
        
        í…ìŠ¤íŠ¸: {output[:1000]}  # í† í° ì ˆì•½
        
        ê²€ì¦ í•­ëª©:
        1. ë¬¸ë²•ì  ì˜¤ë¥˜
        2. ëª…ë°±í•œ ë…¼ë¦¬ ì˜¤ë¥˜
        3. ì„¤ì • ìœ„ë°˜ (ì„¤ì •: {self.current_setting})
        
        ì‹ ë¢°ë„ ì ìˆ˜(0-1)ì™€ í•¨ê»˜ JSON ì‘ë‹µ.
        """
        
        response = self.llm.generate(prompt, max_tokens=200)
        return self._parse_validation(response)
```

## D. ì¦ê±° ê¸°ë°˜ ë¬´ê²°ì„± ì‹œìŠ¤í…œ

### 1. **ë¸”ë¡ì²´ì¸ ìŠ¤íƒ€ì¼ ì˜ìˆ˜ì¦ ì²´ê³„**

```python
class EvidenceChain:
    def __init__(self, storage_path):
        self.storage = storage_path
        self.chain = []
        self.current_hash = None
    
    def create_receipt(self, action_type, data, metadata):
        """ë³€ê²½ ì´ë ¥ ì˜ìˆ˜ì¦ ìƒì„±"""
        
        receipt = {
            "timestamp": datetime.now().isoformat(),
            "action": action_type,
            "data_hash": self._calculate_hash(data),
            "file_path": self._get_physical_path(data),
            "previous_hash": self.current_hash,
            "metadata": metadata,
            "llm_signature": None,
            "system_signature": self._system_sign()
        }
        
        # Geminië¥¼ í†µí•œ ë‚´ìš© ê²€ì¦ (ì„ íƒì )
        if action_type in ["RULE_UPDATE", "CONTENT_MODIFY"]:
            receipt["llm_signature"] = self._get_llm_verification(data)
        
        # í•´ì‹œ ì²´ì¸ ì—…ë°ì´íŠ¸
        receipt["receipt_hash"] = self._calculate_hash(receipt)
        self.current_hash = receipt["receipt_hash"]
        self.chain.append(receipt)
        
        # ë¬¼ë¦¬ì  ì €ì¥ (ë³€ê²½ ë¶ˆê°€)
        self._write_to_immutable_storage(receipt)
        
        return receipt
    
    def verify_integrity(self, data_id):
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""
        
        # í•´ì‹œ ì²´ì¸ ê²€ì¦
        chain_valid = self._verify_hash_chain()
        
        # ë¬¼ë¦¬ì  íŒŒì¼ ì¡´ì¬ í™•ì¸
        file_exists = os.path.exists(self._get_file_path(data_id))
        
        # Gemini ì¬ê²€ì¦ (í™•ë¥ ì )
        if random.random() < 0.05:  # 5% ìƒ˜í”Œë§ ê²€ì¦
            content = self._load_content(data_id)
            llm_verification = self._llm_spot_check(content)
        else:
            llm_verification = True
        
        return all([chain_valid, file_exists, llm_verification])
    
    def _get_llm_verification(self, data):
        """Geminië¥¼ í†µí•œ ë³€ê²½ ê²€ì¦ ì„œëª…"""
        prompt = f"""
        ë‹¤ìŒ ë³€ê²½ ì‚¬í•­ì„ ê²€ì¦í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”:
        
        ë³€ê²½ ì „: {data.get('before', 'N/A')[:300]}
        ë³€ê²½ í›„: {data.get('after', 'N/A')[:300]}
        ë³€ê²½ ì´ìœ : {data.get('reason', 'N/A')}
        
        ë³€ê²½ì´ ì ì ˆí•œì§€ íŒë‹¨í•˜ê³ , 32ìë¦¬ ê²€ì¦ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
        """
        
        response = self.llm.generate(prompt)
        return response[:32]  # 32ìë¦¬ ê²€ì¦ ì½”ë“œ
```

### 2. **ë‹¤ì¤‘ ì„œëª… ë¬´ê²°ì„± ì‹œìŠ¤í…œ**

```python
class MultiSigIntegrity:
    def __init__(self, required_signatures=3):
        self.required = required_signatures
        self.signers = [
            self._hash_based_signer,
            self._semantic_signer,
            self._llm_signer,
            self._statistical_signer
        ]
    
    def validate_modification(self, original, modified, context):
        """ë‹¤ì¤‘ ê²€ì¦ì„ í†µí•œ ë¬´ê²°ì„± ë³´ì¥"""
        
        signatures = []
        
        # ë³‘ë ¬ ê²€ì¦ (ë¹„ë™ê¸°)
        with ThreadPoolExecutor() as executor:
            futures = []
            for signer in self.signers[:self.required]:
                future = executor.submit(signer, original, modified, context)
                futures.append(future)
            
            for future in asyncio.as_completed(futures):
                signature = future.result()
                if signature["valid"]:
                    signatures.append(signature)
                else:
                    # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ ì¦‰ì‹œ ì‹¤íŒ¨
                    return {
                        "valid": False,
                        "failed_signer": signature["signer"],
                        "reason": signature["reason"]
                    }
        
        # ì¶©ë¶„í•œ ì„œëª… íšë“ ì‹œ ì„±ê³µ
        if len(signatures) >= self.required:
            return {
                "valid": True,
                "signatures": signatures,
                "integrity_score": self._calculate_score(signatures)
            }
        
        return {"valid": False, "reason": "Insufficient valid signatures"}
```

## ğŸ¯ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: ê¸°ì´ˆ ì‹œìŠ¤í…œ (2ì£¼)
1. AutoLabelingPipeline êµ¬í˜„
2. ê¸°ë³¸ LawCandidateGenerator êµ¬ì¶•
3. ê°„ë‹¨í•œ EvidenceChain ì„¤ì •

### Phase 2: ê³ ê¸‰ ê¸°ëŠ¥ (3ì£¼)
1. NarrativeDriftDetector ì™„ì„±
2. RegressionAIInspector í†µí•©
3. MultiSigIntegrity ì‹œìŠ¤í…œ ì¶”ê°€

### Phase 3: ìµœì í™” (1ì£¼)
1. ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹± ìµœì í™”
2. Gemini 2.5 Flash-Lite í† í° ìµœì†Œí™”
3. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### Phase 4: íì‡„ ë£¨í”„ (2ì£¼)
1. ìê°€ í•™ìŠµ í”¼ë“œë°± ë£¨í”„ ì—°ê²°
2. ìë™ ë²•ì•ˆ ë°°í¬ ì‹œìŠ¤í…œ
3. ì‹¤ì‹œê°„ ê²½ë³´ ë° ëŒ€ì‘ ìë™í™”

## ğŸ“ˆ ì˜ˆìƒ íš¨ìœ¨ì„±

1. **ë¹„ìš© ì ˆê°**: Gemini 2.5 Flash-Lite ì‚¬ìš©ìœ¼ë¡œ 2.0 Flash ëŒ€ë¹„ 90% ì´ìƒ ë¹„ìš© ì ˆê°
2. **ì²˜ë¦¬ ì†ë„**: ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹±ìœ¼ë¡œ ì²˜ë¦¬ëŸ‰ 3ë°° í–¥ìƒ
3. **ì •í™•ë„**: ë‹¤ì¤‘ ê²€ì¦ ì‹œìŠ¤í…œìœ¼ë¡œ ì˜¤íƒë¥  95% ê°ì†Œ
4. **ìë™í™”ìœ¨**: í˜„ì¬ 30% â†’ ëª©í‘œ 85% ìë™í™”

## âš™ï¸ ì¶”ì²œ ê¸°ìˆ  ìŠ¤íƒ

```yaml
core:
  llm: "gemini-2.5-flash-lite"  # ë¹„ìš© íš¨ìœ¨ì„±
  database: "SQLite + FAISS"     # ì„ë² ë”© ê²€ìƒ‰
  cache: "Redis"                 # ì‹¤ì‹œê°„ ìºì‹±
  queue: "Celery + RabbitMQ"     # ì‘ì—… í
monitoring:
  metrics: "Prometheus"
  logging: "ELK Stack"
  alert: "Grafana + Slack"
deployment:
  orchestration: "Docker + Kubernetes"
  ci/cd: "GitHub Actions"
```

ì´ ì„¤ê³„ì•ˆì€ Gemini 2.5 Flash-Liteì˜ ë¹„ìš© íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ë©´ì„œ, G6X ì‹œìŠ¤í…œì˜ ìê°€ë°œì „ ë£¨í”„ë¥¼ êµ¬í˜„í•˜ëŠ” ì‹¤ìš©ì ì¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤. ê° ì»´í¬ë„ŒíŠ¸ëŠ” ì ì§„ì ìœ¼ë¡œ ë„ì… ê°€ëŠ¥í•˜ë©°, í˜„ì¬ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©ì„ ê³ ë ¤í•˜ì—¬ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.