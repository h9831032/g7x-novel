í´ë¡œë“œ ë‹¤ë³€

ğŸ“˜ ì§ˆë¬¸ 1: ë¡± ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì¶©ëŒ íƒì§€ ì•„í‚¤í…ì²˜

í•µì‹¬ ì² í•™: "ëˆ„ì  ê¸°ì–µ + ì¶©ëŒ íƒì§€ ë ˆì´ì–´"

ë‹¨ìˆœíˆ ê° ì²­í¬ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ê²€ìˆ˜í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ì´ì „ ì²­í¬ë“¤ì˜ 'ì„¤ì • ìƒíƒœ'ë¥¼ ëˆ„ì  ê¸°ì–µìœ¼ë¡œ ìŒ“ì•„ê°€ë©° ìƒˆë¡œìš´ ì²­í¬ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ì¶©ëŒ ì—¬ë¶€ë¥¼ ì²´í¬í•˜ëŠ” êµ¬ì¡°ê°€ í•„ìš”í•©ë‹ˆë‹¤.



ìƒíƒœ ìš”ì•½ ë°ì´í„°(State Summary) JSON ìŠ¤í‚¤ë§ˆ

{

  "chunk_id": 12,

  "timestamp_in_story": "3ì¼ì°¨ ì˜¤í›„",

  

  "characters": {

    "ì´ì¤€í˜": {

      "last_location": "ì„œìš¸ì—­ 3ë²ˆ ì¶œêµ¬",

      "last_seen_chunk": 12,

      "inventory": ["ê²€ì€ ì¹´ë“œ", "ë‚¡ì€ ì—´ì‡ "],

      "status": {

        "health": "ë¶€ìƒ(ì™¼íŒ” ê³¨ì ˆ)",

        "emotional_state": "ë¶„ë…¸",

        "relationship_changes": {

          "ê¹€ìˆ˜ì§„": "ì‹ ë¢°â†’ì˜ì‹¬ (ì²­í¬11)"

        }

      },

      "abilities": ["ì—¼ë ¥(ë ˆë²¨3)", "ìˆœê°„ì´ë™(ë´‰ì¸ì¤‘)"],

      "knowledge_gained": ["ì¡°ì§ì˜ ë°°ì‹ ìëŠ” ë‚´ë¶€ì¸", "ì²­í¬10"]

    }

  },

  

  "world_facts": {

    "magic_system": {

      "mana_exhaustion_rule": "í•˜ë£¨ 3íšŒ ì œí•œ",

      "established_in_chunk": 2,

      "violations_detected": []

    },

    "timeline": {

      "current_date": "2024ë…„ 6ì›” 15ì¼",

      "elapsed_days": 3,

      "major_events": [

        {"chunk": 5, "event": "ì„œìš¸ ë¶•ê´´", "irreversible": true}

      ]

    }

  },

  

  "items_registry": {

    "ê²€ì€ ì¹´ë“œ": {

      "owner": "ì´ì¤€í˜",

      "acquired_chunk": 3,

      "last_used_chunk": 9,

      "properties": ["ì¼íšŒìš©", "íŒŒê´´ë¶ˆê°€"],

      "status": "ì†Œì§€ì¤‘"

    }

  },

  

  "conflict_flags": [

    {

      "type": "LOCATION_TELEPORT",

      "severity": "HIGH",

      "description": "ì²­í¬11ì—ì„œ ë¶€ì‚°ì´ë˜ ìºë¦­í„°ê°€ ì²­í¬12ì—ì„œ ì´ë™ ìˆ˜ë‹¨ ì—†ì´ ì„œìš¸ ë“±ì¥",

      "chunks": [11, 12]

    }

  ]

}

í•µì‹¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•Œê³ ë¦¬ì¦˜ (5ë‹¨ê³„ ê²€ì¦)

1ï¸âƒ£ ì¸ë¬¼ ë™ì„  ì¶”ì  (Spatial Continuity Check)

IF ìºë¦­í„°ì˜ ì´ì „_ìœ„ì¹˜ì™€ í˜„ì¬_ìœ„ì¹˜ ê±°ë¦¬ > ê°€ëŠ¥_ì´ë™ê±°ë¦¬:

  IF í…”ë ˆí¬íŠ¸_ëŠ¥ë ¥ ì—†ìŒ AND ì´ë™_ë¬˜ì‚¬ ì—†ìŒ:

    â†’ FLAG "ë¬¼ë¦¬ì  ì´ë™ ë¶ˆê°€ëŠ¥"

íš¨ìœ¨í™” íŒ: ì§€ëª…ì„ í‘œì¤€í™”ëœ ì¢Œí‘œê³„ë¡œ ë³€í™˜ (ì˜ˆ: "ê°•ë‚¨ì—­" â†’ {lat: 37.498, lng: 127.027}) í›„ ê±°ë¦¬ ê³„ì‚°.



2ï¸âƒ£ ì•„ì´í…œ ì†Œì§€ ì¶”ì  (Inventory Consistency)

FOR ê° ì•„ì´í…œ IN ì‚¬ìš©_ë˜ëŠ”_ì–¸ê¸‰:

  IF ì•„ì´í…œ.owner != í˜„ì¬_ì‚¬ìš©ì:

    â†’ FLAG "ì†Œìœ ê¶Œ ë¶ˆì¼ì¹˜"

  IF ì•„ì´í…œ.status == "íŒŒê´´ë¨" AND í˜„ì¬_ì²­í¬ > íŒŒê´´_ì²­í¬:

    â†’ FLAG "íŒŒê´´ëœ ì•„ì´í…œ ì¬ë“±ì¥"

JSON ì„¤ê³„ í¬ì¸íŠ¸: ì•„ì´í…œì— unique_idë¥¼ ë¶€ì—¬í•˜ê³ , "ë§ˆë‚˜ í¬ì…˜"ì´ë¼ëŠ” ì¼ë°˜ëª…ì‚¬ê°€ ì•„ë‹Œ "ë§ˆë‚˜ í¬ì…˜#0042" ê°™ì€ ê³ ìœ  ì‹ë³„ìë¡œ ê´€ë¦¬.



3ï¸âƒ£ ëŠ¥ë ¥ ì‹œìŠ¤í…œ ì¼ê´€ì„± (Power Scaling Check)

IF ìºë¦­í„°ê°€ ì´ì „ì—_ì‹¤íŒ¨í•œ_í–‰ë™ì„ ì„±ê³µ:

  IF ëŠ¥ë ¥_ìƒìŠ¹_ì´ë²¤íŠ¸ ì—†ìŒ:

    â†’ FLAG "íŒŒì›Œ ì¸í”Œë ˆì´ì…˜ ì˜ì‹¬"

  

IF ëŠ¥ë ¥_ì‚¬ìš© > ì„¤ì •ëœ_ì œí•œ:

  â†’ FLAG "ì‹œìŠ¤í…œ ë£° ìœ„ë°˜" (ì˜ˆ: í•˜ë£¨ 3íšŒ ì œí•œì¸ë° 4ë²ˆì§¸ ì‚¬ìš©)

4ï¸âƒ£ ì‹œê°„ì„  ê²€ì¦ (Temporal Paradox Detection)

IF í˜„ì¬_ìŠ¤í† ë¦¬_ì‹œê°„ < ê³¼ê±°_ì´ë²¤íŠ¸_ì‹œê°„:

  â†’ FLAG "ì‹œê°„ ì—­í–‰"

  

IF ê²½ê³¼_ì‹œê°„ì´ ìƒë¦¬í•™ì ìœ¼ë¡œ_ë¶ˆê°€ëŠ¥:

  (ì˜ˆ: 2ì‹œê°„ ë™ì•ˆ 8ì‹œê°„ì¹˜ í–‰ë™)

  â†’ FLAG "ì‹œê°„ ì••ì¶• ì˜¤ë¥˜"

5ï¸âƒ£ ì§€ì‹ ìƒíƒœ ì¶”ì  (Knowledge Propagation)

IF ìºë¦­í„°Aê°€ ì •ë³´Xë¥¼ ëª¨ë¥´ëŠ”_ìƒíƒœë¡œ_í–‰ë™:

  IF ì •ë³´XëŠ” ì´ì „_ì²­í¬ì—ì„œ_ìŠµë“ë¨:

    â†’ FLAG "ìºë¦­í„° ê¸°ì–µìƒì‹¤"

ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì œì•ˆ

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ Chunk N-1   â”‚â”€â”€â†’ State Summary N-1

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â†“

                  [ëˆ„ì  ë©”ëª¨ë¦¬ DB]

                        â†“

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ Chunk N     â”‚â”€â”€â”€â†’â”‚ Conflict     â”‚

â”‚ (ì‹ ê·œ ì…ë ¥)  â”‚    â”‚ Detector AI  â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        â†“

              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

              â”‚ â€¢ ê¸°ì¡´ ì„¤ì • ë¡œë“œ  â”‚

              â”‚ â€¢ ì‹ ê·œ ì •ë³´ ì¶”ì¶œ  â”‚

              â”‚ â€¢ ì¶©ëŒ íƒì§€       â”‚

              â”‚ â€¢ State ì—…ë°ì´íŠ¸  â”‚

              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        â†“

                State Summary N â”€â”€â†’ [DB ì €ì¥]

í•µì‹¬: ê° ì²­í¬ ì²˜ë¦¬ ì‹œ AIì—ê²Œ ë‹¤ìŒì„ ë™ì‹œì— ì œê³µ



í˜„ì¬ ì²­í¬ ì›ë¬¸

ëˆ„ì ëœ State Summary (ìµœëŒ€ ìµœê·¼ 20ê°œ ì²­í¬)

"ì¶©ëŒ íƒì§€ í”„ë¡¬í”„íŠ¸" (ì˜ˆ: "ì´ ì¥ë©´ì—ì„œ ìºë¦­í„°ê°€ ì‚¬ìš©í•œ ì•„ì´í…œì´ ì´ì „ì— íŒŒê´´ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ë¼")

ğŸ“ ì§ˆë¬¸ 2: ë¬¸ì¥ ì„í™” & ë§íˆ¬ ë“œë¦¬í”„íŠ¸ íƒì§€

ì„í™” ì§€ìˆ˜(Stone Score) ê³„ì‚° ê¸°ì¤€ 5ê°€ì§€

1ï¸âƒ£ í˜¸í¡ ë°€ë„ (Breath Density) [ì •ëŸ‰]

ì„í™”ë„ = (ìˆ˜ë™íƒœë¬¸ì¥ + ê´€í˜•ì ˆ 3ë‹¨ì´ìƒ + 150ìì´ìƒ ë¬¸ì¥) / ì „ì²´ë¬¸ì¥ìˆ˜ Ã— 100



ì˜ˆì‹œ:

âŒ ì„í™” 90ì : "ê·¸ê°€ ë§í–ˆë˜, ì–´ì œ ë°¤ ë¹„ê°€ ë‚´ë¦¬ë˜ ê·¸ ìˆœê°„ì— ëª©ê²©í–ˆë˜ ê²ƒìœ¼ë¡œ ì¶”ì •ë˜ëŠ” ì‚¬ê±´ì˜ ì „ë§ì€..."

âœ… ì„í™” 10ì : "ê·¸ê°€ ë´¤ë‹¤. ì–´ì ¯ë°¤, ë¹„ ë‚´ë¦¬ë˜ ìˆœê°„ì„."

ì¸¡ì •ë²•: ë¬¸ì¥ë‹¹ í‰ê·  ê¸€ì ìˆ˜, ì¢…ì†ì ˆ ê¹Šì´(dependency depth), ëª…ì‚¬í™” ë¹„ìœ¨ì„ ì¢…í•©.



2ï¸âƒ£ ê°ê° ì¶”ìƒë„ (Sensory Abstractness) [ì •ì„±+ì •ëŸ‰]

ì„í™”ë„ = ì¶”ìƒëª…ì‚¬ ë°€ë„ - ê°ê°ë™ì‚¬ ë°€ë„



ì¶”ìƒëª…ì‚¬: í¬ë§, ì ˆë§, ì˜ì§€, ìš´ëª…, ë³¸ì§ˆ...

ê°ê°ë™ì‚¬: ì›€ì¼œì¥ë‹¤, í›„ë ¤ì¹˜ë‹¤, ë¹„í‹€ë‹¤, ì‚¼í‚¤ë‹¤...



âŒ ì„í™” 85ì : "ê·¸ì˜ ë‚´ë©´ì—ëŠ” ë³µìˆ˜ì— ëŒ€í•œ ê°•ë ¬í•œ ì˜ì§€ê°€ ì¡´ì¬í–ˆë‹¤"

âœ… ì„í™” 15ì : "ê·¸ì˜ ì†í†±ì´ ì†ë°”ë‹¥ì„ íŒŒê³ ë“¤ì—ˆë‹¤"

3ï¸âƒ£ ì„¤ëª…/ë¬˜ì‚¬ ë¹„ìœ¨ (Tell-Show Ratio) [ì •ì„±]

ì„í™”ë„ = (ì„¤ëª…ë¬¸ / ì¥ë©´ë¬˜ì‚¬ë¬¸) Ã— 100



ì„¤ëª…ë¬¸ íŒ¨í„´:

- "~ì˜€ë‹¤/ì´ì—ˆë‹¤" + ìƒíƒœì„¤ëª…

- "~í•˜ëŠ” ì‚¬ëŒì´ì—ˆë‹¤"

- "ê·¸ ì´ìœ ëŠ” ~ë•Œë¬¸ì´ë‹¤"



âŒ ì„í™” 95ì : "ì¤€í˜ì€ í™”ê°€ ë‚œ ìƒíƒœì˜€ë‹¤"

âœ… ì„í™” 5ì : "ì¤€í˜ì˜ ê´€ìë†€ì´ì— í•ì¤„ì´ íŠ€ì–´ì˜¬ëë‹¤"

4ï¸âƒ£ ë¬¸ì¥ ê°œì‹œë¶€ íŒ¨í„´ (Opening Staleness) [ì •ëŸ‰]

ì„í™”ë„ = ë™ì¼ ê°œì‹œíŒ¨í„´ ì—°ì† ì¶œí˜„ íšŸìˆ˜ Ã— 20



ì—°ì† 3ë¬¸ì¥ì´ "ê·¸ëŠ”..."ìœ¼ë¡œ ì‹œì‘ â†’ 60ì 

ì—°ì† 5ë¬¸ì¥ì´ "~ì˜€ë‹¤" ì¢…ê²° â†’ 100ì 



âœ… ë¦¬ë“¬ ì¢‹ì€ ì˜ˆ:

"ì¹¼ë‚ ì´ ì„¬ê´‘ì„ ê·¸ë ¸ë‹¤. ê·¸ê°€ ë¹„í‹€ê±°ë ¸ë‹¤. í”¼ê°€ íŠ€ì—ˆë‹¤."

(ì£¼ì–´ ë³€í™” + ì¢…ê²° ë‹¤ì–‘ì„±)

5ï¸âƒ£ ì¥ë¥´ ê´€ìŠµ ì´íƒˆë„ (Genre Deviation) [ì •ì„±]

ì›¹ì†Œì„¤(íŠ¹íˆ íŒíƒ€ì§€/ì•¡ì…˜)ì˜ ê¸°ì¤€ ë¬¸ì²´ íŠ¹ì„±:

- í‰ê·  ë¬¸ì¥ ê¸¸ì´: 25-40ì

- ëŠ¥ë™íƒœ ë¹„ìœ¨: 85% ì´ìƒ

- ëŒ€í™”/ì§€ë¬¸ ë¹„ìœ¨: 4:6 ~ 6:4



ì„í™”ë„ = |í˜„ì¬ë¬¸ì²´ - ì¥ë¥´í‘œì¤€| ì˜ ê±°ë¦¬

ì¸¡ì • ì˜ˆì‹œ:



ë¬¸í•™ì†Œì„¤ì²˜ëŸ¼ 70ìì§œë¦¬ ë¬¸ì¥ ì—°ì† 3ê°œ â†’ ì„í™” 80ì 

ì„¤ëª…ì¡° 2,000ì í›„ ëŒ€í™” í•œ ì¤„ â†’ ì„í™” 90ì 

ë¬¸ì²´ ì§€ë¬¸(Stylistic Fingerprint) ì¶”ì¶œ ë°©ë²•

í†µê³„ì  íŠ¹ì§• ë²¡í„° (7ì°¨ì›)

StyleFingerprint = {

  # 1. ì–´íœ˜ ë‹¤ì–‘ì„± (Lexical)

  "ttr": len(unique_words) / len(total_words),  # 0.6~0.75ê°€ ì›¹ì†Œì„¤ í‰ê· 

  "rare_word_ratio": count(freq < 100) / total_words,

  

  # 2. ë¬¸ì¥ ë¦¬ë“¬ (Syntactic)

  "avg_sentence_length": mean([len(s) for s in sentences]),

  "sentence_length_variance": std([len(s) for s in sentences]),

  

  # 3. í’ˆì‚¬ ë¶„í¬ (Morphological)

  "verb_noun_ratio": count(ë™ì‚¬) / count(ëª…ì‚¬),

  "adjective_density": count(í˜•ìš©ì‚¬) / total_words,

  

  # 4. ì¢…ê²°ì–´ë¯¸ íŒ¨í„´ (Stylometric)

  "ending_patterns": Counter([s[-2:] for s in sentences]),

  # ì˜ˆ: {"ì—ˆë‹¤": 0.4, "ëŠ”ë‹¤": 0.3, "ã„´ê°€": 0.1...}

  

  # 5. ëŒ€í™”/ì§€ë¬¸ ë¹„ìœ¨

  "dialogue_ratio": len(ëŒ€í™”ë¬¸) / len(ì „ì²´í…ìŠ¤íŠ¸),

  

  # 6. ê°ì • ê°•ë„ ë¶„í¬

  "emotion_intensity": mean([ê°ì •ì–´íœ˜_ê°•ë„ì ìˆ˜]),

  

  # 7. ì ‘ì†ì‚¬ ì‚¬ìš© íŒ¨í„´

  "conjunction_signature": [ê·¸ë¦¬ê³ _ë¹ˆë„, í•˜ì§€ë§Œ_ë¹ˆë„, ê·¸ëŸ¬ë‚˜_ë¹ˆë„...]

}

ë§íˆ¬ ë“œë¦¬í”„íŠ¸ íƒì§€ ì•Œê³ ë¦¬ì¦˜

1. ì‘ê°€ ê¸°ì¤€ì„  ì„¤ì •:

   - ì²« 3ê°œ ì²­í¬ì˜ í‰ê·  ì§€ë¬¸ = Baseline



2. ê° ì²­í¬ë§ˆë‹¤:

   Drift_Score = cosine_distance(í˜„ì¬ì§€ë¬¸, Baseline)

   

3. ì„ê³„ì¹˜ ì„¤ì •:

   IF Drift_Score > 0.25:  # ì½”ì‚¬ì¸ ê±°ë¦¬ ê¸°ì¤€

     â†’ "ë¬¸ì²´ ê¸‰ë³€ ê°ì§€"

     â†’ ì„¸ë¶€ ë¶„ì„: ì–´ë–¤ ì°¨ì›ì´ íŠ€ì—ˆëŠ”ì§€ ë¦¬í¬íŠ¸



4. ìºë¦­í„° ë§íˆ¬ ì „ìš© ê²€ì‚¬:

   ê° ìºë¦­í„°ì˜ ëŒ€í™”ë¬¸ë§Œ ì¶”ì¶œ â†’ ë³„ë„ ì§€ë¬¸ ìƒì„±

   

   ì˜ˆ: "ê¹€ìˆ˜ì§„" ë§íˆ¬ ì§€ë¬¸

   {

     "sentence_enders": ["~ìš”": 0.6, "~ì£ ": 0.2],  # ì¡´ëŒ“ë§ ìºë¦­í„°

     "filler_words": ["ê·¸ëŸ¬ë‹ˆê¹Œ", "ìˆì–ì•„"],

     "avg_sentence_length": 18  # ì§§ê²Œ ëŠì–´ ë§í•¨

   }

   

   IF ì²­í¬15ì—ì„œ ìˆ˜ì§„ì´ ê°‘ìê¸° "~ì´ë‹¤" ì¢…ê²° ì‚¬ìš©:

     â†’ "ìºë¦­í„° ë§íˆ¬ ì´íƒˆ" í”Œë˜ê·¸

ì‹¤ì „ ì ìš© ì˜ˆì‹œ

ì‘ê°€ì˜ Baseline ì§€ë¬¸:



- í‰ê·  ë¬¸ì¥ê¸¸ì´: 32ì

- ë™ì‚¬/ëª…ì‚¬ ë¹„ìœ¨: 1.2

- "ì—ˆë‹¤" ì¢…ê²°: 45%



ì²­í¬ 47ì—ì„œ ê°ì§€ëœ ìˆ˜ì¹˜:

- í‰ê·  ë¬¸ì¥ê¸¸ì´: 58ì â† âš ï¸ 81% ì¦ê°€

- ë™ì‚¬/ëª…ì‚¬ ë¹„ìœ¨: 0.7 â† âš ï¸ 41% ê°ì†Œ

- "ìˆì—ˆë‹¤" ì¢…ê²°: 65% â† âš ï¸ íŒ¨í„´ ë³€í™”



â†’ Drift Score: 0.34 (High)

â†’ ì§„ë‹¨: "ì„¤ëª…ì¡° ë¬¸ì²´ë¡œ ê¸‰ë³€, ëª…ì‚¬êµ¬ ê³¼ë‹¤ ì‚¬ìš©"

ğŸ¯ í†µí•© ì‹œìŠ¤í…œ ê¶Œì¥ì‚¬í•­

ë‘ ê¸°ëŠ¥ì„ ê²°í•©í•œë‹¤ë©´:



1ì°¨ íŒ¨ìŠ¤: ë¡± ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì¶©ëŒ ê²€ì‚¬ (ë…¼ë¦¬/ì„¤ì •)

2ì°¨ íŒ¨ìŠ¤: ë¬¸ì¥ ì„í™” + ë¬¸ì²´ ë“œë¦¬í”„íŠ¸ ê²€ì‚¬ (ë¬¸ì²´/ê°ê°)

ì´ë ‡ê²Œ ë¶„ë¦¬í•˜ë©´ AIê°€ ê° ì‘ì—…ì— ì§‘ì¤‘í•  ìˆ˜ ìˆì–´ ì •í™•ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤. Gemini 2.0 FlashëŠ” 1ì°¨ íŒ¨ìŠ¤ì—, ë” ì„¸ë°€í•œ ë¬¸ì²´ ë¶„ì„ì€ Claude ê°™ì€ ëª¨ë¸ì— ë§¡ê¸°ëŠ” í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ë„ ê³ ë ¤í•´ë³´ì„¸ìš”.

êµ¬í˜„í•˜ë©´ì„œ ë§‰íˆëŠ” ë¶€ë¶„ ìˆìœ¼ë©´ ì–¸ì œë“  ë‹¤ì‹œ ë¬¼ì–´ë³´ì„¸ìš”! í´ë¡œë“œ ë‹µë³€ 




ë”¥ì‹œí¬ë‹µë³€ì´ë‹¤.

# ì›¹ì†Œì„¤ ê²€ìˆ˜ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì œì•ˆ

## 1. ë¡± ì»¨í…ìŠ¤íŠ¸ ì„¤ì • ì¶©ëŒ ê°ì§€ ì‹œìŠ¤í…œ

### ìƒíƒœ ìš”ì•½ ë°ì´í„°(State Summary) ì•„í‚¤í…ì²˜

```json
{
  "system_id": "webnovel_validator_v1.0",
  "chunk_analysis": {
    "current_chunk": 12,
    "state_summary": {
      "characters": {
        "character_id_1": {
          "name": "ê¹€ì² ìˆ˜",
          "current_location": "ë˜ì „ 3ì¸µ ë³´ìŠ¤ë£¸",
          "previous_locations": ["ë§ˆì„ ê´‘ì¥", "ë˜ì „ ì…êµ¬", "ë˜ì „ 2ì¸µ"],
          "inventory": ["ë§ˆë²• ê²€", "ì¹˜ìœ  í¬ì…˜ 3ê°œ", "ê³ ëŒ€ ì—´ì‡ "],
          "relationships": {
            "character_id_2": "ë™ë£Œ",
            "character_id_3": "ì ëŒ€"
          },
          "status_effects": ["í”¼ë¡œë„ 40%", "ë§ˆë‚˜ 30%"],
          "last_mentioned_chunk": 11,
          "consistency_flags": []
        }
      },
      "world_state": {
        "time_progression": {
          "current_time": "ë˜ì „ íƒìƒ‰ 3ì¼ì°¨",
          "time_consistency_score": 0.95
        },
        "key_events": [
          {
            "event_id": "e001",
            "description": "ë§ˆë²• ê²€ íšë“",
            "occurred_chunk": 3,
            "referenced_in": [5, 8, 10]
          }
        ],
        "established_rules": [
          {
            "rule_id": "r001",
            "description": "ë§ˆë²• ê²€ì€ í•˜ë£¨ì— 3ë²ˆë§Œ ì‚¬ìš© ê°€ëŠ¥",
            "established_chunk": 3,
            "last_verified": 10
          }
        ]
      },
      "item_tracking": {
        "items_in_play": [
          {
            "item_id": "i001",
            "name": "ë§ˆë²• ê²€",
            "current_owner": "character_id_1",
            "ownership_history": [
              {"chunk": 3, "owner": "character_id_1", "event": "íšë“"}
            ],
            "properties": ["í•˜ë£¨ 3íšŒ ì‚¬ìš© ì œí•œ", "ë¹› ì†ì„±"]
          }
        ],
        "consumed_items": []
      },
      "location_continuity": {
        "active_locations": ["ë˜ì „ 3ì¸µ"],
        "character_distribution": {
          "ë˜ì „ 3ì¸µ": ["character_id_1", "character_id_2"],
          "ë§ˆì„": ["character_id_3"]
        }
      }
    }
  },
  "consistency_checklist": {
    "applied_rules": [
      "RULE_001: ìºë¦­í„° ìœ„ì¹˜ ì¶”ì ",
      "RULE_002: ì•„ì´í…œ ì†Œì§€ê¶Œ ì´ì „ ê²€ì¦",
      "RULE_003: ì„¤ì • ê·œì¹™ ìœ„ë°˜ íƒì§€",
      "RULE_004: ì‹œê°„ ì„ í˜•ì„± í™•ì¸"
    ],
    "detected_anomalies": [],
    "confidence_score": 0.92
  }
}
```

### í•µì‹¬ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì•Œê³ ë¦¬ì¦˜

```python
class WebnovelConsistencyValidator:
    def __init__(self):
        self.state_history = []
        self.checkpoints = {}
        
    def extract_state_summary(self, chunk_text, chunk_id):
        """ì²­í¬ì—ì„œ ìƒíƒœ ì •ë³´ ì¶”ì¶œ"""
        summary = {
            "chunk_id": chunk_id,
            "character_movements": self.extract_character_movements(chunk_text),
            "item_transactions": self.extract_item_transactions(chunk_text),
            "rule_establishments": self.extract_new_rules(chunk_text),
            "time_advancements": self.extract_time_changes(chunk_text),
            "dialogue_consistency": self.analyze_dialogue_consistency(chunk_text)
        }
        return summary
    
    def validate_against_history(self, current_summary, previous_states):
        """ì—­ì‚¬ì  ìƒíƒœì™€ ë¹„êµí•˜ì—¬ ì¶©ëŒ íƒì§€"""
        conflicts = []
        
        # 1. ì¸ë¬¼ ë™ì„  ì¶©ëŒ ê²€ì‚¬
        for char_id, movement in current_summary["character_movements"].items():
            previous_location = self.get_previous_location(char_id, previous_states)
            if previous_location and not self.is_movement_plausible(
                previous_location, movement["current_location"], 
                movement["time_elapsed"]
            ):
                conflicts.append({
                    "type": "CHARACTER_TELEPORTATION",
                    "character": char_id,
                    "previous": previous_location,
                    "current": movement["current_location"],
                    "chunk_range": f"{self.find_last_mention(char_id)}-{current_summary['chunk_id']}"
                })
        
        # 2. ì•„ì´í…œ ì†Œì§€ ì¶©ëŒ ê²€ì‚¬
        for item_transaction in current_summary["item_transactions"]:
            if not self.validate_item_ownership(item_transaction, previous_states):
                conflicts.append({
                    "type": "ITEM_OWNERSHIP_CONFLICT",
                    "item": item_transaction["item_id"],
                    "claimed_owner": item_transaction["new_owner"],
                    "actual_owner": self.get_true_owner(item_transaction["item_id"]),
                    "chunk": current_summary["chunk_id"]
                })
        
        # 3. ì„¤ì • ê·œì¹™ ì¶©ëŒ ê²€ì‚¬
        for rule in current_summary["rule_establishments"]:
            conflicting_rule = self.find_rule_conflict(rule, previous_states)
            if conflicting_rule:
                conflicts.append({
                    "type": "RULE_CONTRADICTION",
                    "new_rule": rule["description"],
                    "existing_rule": conflicting_rule["description"],
                    "chunks": f"{conflicting_rule['established_chunk']} vs {current_summary['chunk_id']}"
                })
        
        return conflicts
    
    def is_movement_plausible(self, from_loc, to_loc, time_elapsed):
        """ì´ë™ ê°€ëŠ¥ì„± ê²€ì¦ ì•Œê³ ë¦¬ì¦˜"""
        # ìœ„ì¹˜ ê°„ ê±°ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ì™€ ì‹œê°„ ê²½ê³¼ ë¹„êµ
        distance = self.calculate_location_distance(from_loc, to_loc)
        max_speed = 5  # km/h, ê¸°ë³¸ ì¸ê°„ ì´ë™ ì†ë„
        plausible_distance = max_speed * time_elapsed
        
        return distance <= plausible_distance
```

## 2. ë¬¸ì¥ ì„í™”(Petrification) ë° ë§íˆ¬ ë“œë¦¬í”„íŠ¸(Tone Drift) ê°ì§€ ì‹œìŠ¤í…œ

### ì„í™” ì§€ìˆ˜(Stone Score) í‰ê°€ ê¸°ì¤€ 5ê°€ì§€

1. **ë¬¸ì¥ êµ¬ì¡° ë‹¤ì–‘ì„± ì§€ìˆ˜ (0-25ì )**
   - í‰ê·  ë¬¸ì¥ ê¸¸ì´ì˜ í¸ì°¨
   - ë¬¸ì¥ ì‹œì‘ íŒ¨í„´ì˜ ë°˜ë³µë¥  (ì£¼ì–´+ì¡°ì‚¬ ì‹œì‘ ë¹ˆë„)
   - ì ‘ì†ì‚¬/ê´€ê³„ì‚¬ ì‚¬ìš© ë‹¤ì–‘ì„±

2. **ë¦¬ë“¬ íŒŒê´´ ì§€ìˆ˜ (0-20ì )**
   - 3ìŒì ˆ ì´ìƒì˜ í•œìì–´/ì™¸ë˜ì–´ ì—°ì† ì‚¬ìš© ë¹ˆë„
   - ì„¤ëª…ì  ìˆ˜ì‹ì–´ì˜ ê³¼ë„í•œ ì¤‘ì²©
   - ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ ë°©í•´ ìš”ì†Œ ê°ì§€

3. **ìƒë™ê° ì €í•˜ ì§€ìˆ˜ (0-25ì )**
   - ê°ê°ì  í‘œí˜„ ëŒ€ ì¶”ìƒì  í‘œí˜„ ë¹„ìœ¨
   - ë™ì‚¬ ëŒ€ í˜•ìš©ì‚¬/ë¶€ì‚¬ ì‚¬ìš© ë¹„ìœ¨
   - ì§ì ‘ì  í–‰ë™ ì„œìˆ  ëŒ€ ê°„ì ‘ì  ì„¤ëª… ë¹„ìœ¨

4. **ì¥ë©´ ì „í™˜ ìì—°ìŠ¤ëŸ¬ì›€ (0-15ì )**
   - ì¥ë©´ ì „í™˜ ì‹œ ì‹œê°„/ê³µê°„ ë‹¨ì ˆ ì •ë„
   - ìºë¦­í„° ì‚¬ê³  íë¦„ì˜ ì—°ê²°ì„±
   - ë°°ê²½ ì„¤ëª… ì‚½ì…ì˜ ì ì ˆì„±

5. **ìºë¦­í„° ìŒì„± ì¼ê´€ì„± (0-15ì )**
   - ìºë¦­í„°ë³„ ê³ ìœ  ì–´íœ˜ ì‚¬ìš© ì¼ê´€ì„±
   - ëŒ€í™”ì²´ ì„œìˆ ì–´ ì¼ì¹˜ ì—¬ë¶€
   - ë¹„ìœ /ì€ìœ ì˜ ìºë¦­í„° ì í•©ì„±

### ë¬¸ì²´ ì§€ë¬¸(Stylistic Fingerprint) ì¶”ì¶œ ì•Œê³ ë¦¬ì¦˜

```python
class StylisticFingerprintExtractor:
    def __init__(self, author_reference_texts):
        self.author_baseline = self.establish_baseline(author_reference_texts)
        
    def establish_baseline(self, texts):
        """ì‘ê°€ ê¸°ì¤€ ë¬¸ì²´ íŒ¨í„´ í™•ë¦½"""
        baseline = {
            "lexical_patterns": {
                "preferred_sentence_length": self.avg_sentence_length(texts),
                "vocabulary_richness": self.calculate_ttr(texts),  # Type-Token Ratio
                "signature_words": self.extract_signature_ngrams(texts, n=2)
            },
            "syntactic_patterns": {
                "sentence_opening_patterns": self.analyze_sentence_beginnings(texts),
                "connector_frequency": self.count_connectors(texts),
                "modification_depth": self.avg_modification_depth(texts)
            },
            "rhythmic_patterns": {
                "comma_frequency": self.commas_per_sentence(texts),
                "paragraph_transition_style": self.analyze_paragraph_transitions(texts)
            },
            "semantic_patterns": {
                "metaphor_density": self.metaphors_per_1000_chars(texts),
                "sensory_word_ratio": self.sensory_word_ratio(texts),
                "dialogue_narration_ratio": self.dialogue_ratio(texts)
            }
        }
        return baseline
    
    def detect_style_drift(self, current_chunk, previous_chunks):
        """ë¬¸ì²´ ë“œë¦¬í”„íŠ¸ íƒì§€"""
        current_fingerprint = self.extract_fingerprint(current_chunk)
        
        # 1. ì–´íœ˜ì  ë³€ì´ ê°ì§€
        lexical_drift = self.calculate_cosine_similarity(
            current_fingerprint["lexical_patterns"],
            self.author_baseline["lexical_patterns"]
        )
        
        # 2. êµ¬ë¬¸ì  ë³€ì´ ê°ì§€
        syntactic_drift = self.analyze_syntactic_deviation(
            current_fingerprint["syntactic_patterns"]
        )
        
        # 3. ë¦¬ë“¬ íŒ¨í„´ ë³€ì´
        rhythmic_drift = self.compare_rhythmic_patterns(
            current_fingerprint["rhythmic_patterns"],
            previous_chunks[-5:] if len(previous_chunks) >= 5 else previous_chunks
        )
        
        # 4. ì ì§„ì  ë³€í™” vs ê¸‰ê²©í•œ ë³€í™” êµ¬ë¶„
        drift_trend = self.analyze_drift_trend(previous_chunks)
        
        return {
            "overall_drift_score": self.combine_drift_scores(
                lexical_drift, syntactic_drift, rhythmic_drift
            ),
            "drift_components": {
                "lexical": lexical_drift,
                "syntactic": syntactic_drift,
                "rhythmic": rhythmic_drift
            },
            "likely_causes": self.infer_drift_causes(current_chunk),
            "confidence": 0.88
        }
    
    def infer_drift_causes(self, text):
        """ë¬¸ì²´ ë³€ì´ ì›ì¸ ì¶”ë¡ """
        causes = []
        
        if self.detect_ai_assistance_patterns(text):
            causes.append("AI ë³´ì¡° ë„êµ¬ ì‚¬ìš© ê°€ëŠ¥ì„±")
        if self.detect_fatigue_patterns(text):
            causes.append("ì‘ê°€ í”¼ë¡œë„ ìƒìŠ¹ ì§•í›„")
        if self.detect_genre_shift(text):
            causes.append("ì¥ë¥´ ì „í™˜ ì‹œë„")
        if self.detect_multiple_author_patterns(text):
            causes.append("ê³µë™ ì§‘í•„ ê°€ëŠ¥ì„±")
            
        return causes
```

### í†µí•© ê²€ìˆ˜ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
ì›¹ì†Œì„¤ AI ê²€ìˆ˜ íŒŒì´í”„ë¼ì¸:
1. ì²­í¬ ë¶„í•  (5,000ì ë‹¨ìœ„)
   â†“
2. ë¬¸ì²´ ì§€ë¬¸ ë¶„ì„ ë° ì„í™” ì§€ìˆ˜ ê³„ì‚°
   â†“
3. ìƒíƒœ ìš”ì•½ ë°ì´í„° ì¶”ì¶œ (ì¸ë¬¼/ì•„ì´í…œ/ê³µê°„/ì‹œê°„)
   â†“
4. ë¡± ì»¨í…ìŠ¤íŠ¸ ì¶©ëŒ ê²€ì‚¬ (ì—­ì‚¬ì  ìƒíƒœ ë¹„êµ)
   â†“
5. í†µí•© ë¦¬í¬íŠ¸ ìƒì„±:
   - ì¦‰ì‹œ ìˆ˜ì • í•„ìš”: ì„¤ì • ì¶©ëŒ, ë¬¸ì²´ ë¶•ê´´
   - ê²½ê³ : ê²½ë¯¸í•œ ì¼ê´€ì„± ë¬¸ì œ
   - ê´€ì°°: ë¬¸ì²´ ë³€í™” ì¶”ì´
```

ì´ ì‹œìŠ¤í…œì€ ê° ì²­í¬ ë¶„ì„ ì‹œ ì´ì „ ëª¨ë“  ì²­í¬ì˜ ìƒíƒœë¥¼ ì™„ì „íˆ ì¬ë¶„ì„í•˜ì§€ ì•Šê³ ë„, íš¨ìœ¨ì ì¸ ìƒíƒœ ìš”ì•½ ë°ì´í„°ë¥¼ í†µí•´ ë¡± ì»¨í…ìŠ¤íŠ¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, ë™ì‹œì— ì‘ê°€ì˜ ë¬¸ì²´ ì¼ê´€ì„±ì„ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. JSON ìŠ¤í‚¤ë§ˆëŠ” íŠ¹íˆ ì¸ë¬¼ ë™ì„ ê³¼ ì•„ì´í…œ ì¶”ì ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

