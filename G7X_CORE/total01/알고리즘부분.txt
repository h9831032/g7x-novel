# ê°œì—°ì„±/ì¼ë°˜ ì˜¤ë¥˜ ì„¼ì„œ ì‹œìŠ¤í…œ ì„¤ê³„

---

## ğŸ¯ ì „ì²´ ì „ëµ

```
ìˆ˜ë°± ê¶Œ ì†Œì„¤ â†’ 14ê°œ ì„¼ì„œë¡œ ì¸¡ì • â†’ ì •ìƒ ë¶„í¬ ìƒì„± â†’ ìƒˆ í…ìŠ¤íŠ¸ ì´ìƒ íƒì§€
```

---

# PART 1: 14ê°œ ì„¼ì„œ ì •ì˜ (ê¸°ì¡´ 8ê°œ + ì¶”ì²œ 6ê°œ)

## âœ… ê¸°ì¡´ í™•ì • ì„¼ì„œ (8ê°œ)

### 1. ì–´íœ˜ ë‹¤ì–‘ì„± (Lexical Diversity)
```python
def sensor_lexical_diversity(text_block):
    """
    TTR (Type-Token Ratio) ì¶”ì„¸ ì¸¡ì •
    ì„í™” ê°ì§€ìš©ì´ì§€ë§Œ ê°œì—°ì„±ì—ë„ ì˜í–¥
    """
    tokens = tokenize(text_block)
    unique = set(tokens)
    
    return {
        "ttr": len(unique) / len(tokens),
        "mtld": calculate_mtld(tokens),  # Measure of Textual Lexical Diversity
        "trend": "declining" if ttrê°€_3ë¸”ë¡_ì—°ì†_í•˜ë½ else "normal"
    }
```

### 2. ìš”ì•½ ë³€í™”ëŸ‰ (Summary Delta)
```python
def sensor_summary_change(block1, block2):
    """
    ì—°ì† ë¸”ë¡ì˜ ìš”ì•½ ê°„ ë³€í™” ì •ë„
    ë³€í™” ì—†ìœ¼ë©´ ì„í™”, ê¸‰ë³€í•˜ë©´ ë“œë¦¬í”„íŠ¸
    """
    summary1 = api_summarize(block1, max_words=50)
    summary2 = api_summarize(block2, max_words=50)
    
    # ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„
    sim = cosine_similarity(embed(summary1), embed(summary2))
    
    return {
        "similarity": sim,
        "change_rate": 1 - sim,
        "judgment": "stagnant" if sim > 0.9 else "normal" if sim > 0.5 else "drift"
    }
```

### 3. ìƒíƒœ ë³€í™” ì´ë²¤íŠ¸ ë°€ë„ (State Change Density)
```python
def sensor_state_change_density(text_block):
    """
    'íšë“/ì†ì‹¤/ì´ë™/ì „íˆ¬/ë¶€ìƒ/ê²°ì •' ê°™ì€ ìƒíƒœ ë³€í™” ë¹ˆë„
    ë°€ë„ ë‚®ìœ¼ë©´ ì„í™”, ë„ˆë¬´ ë†’ìœ¼ë©´ ì••ì¶• ê³¼ë‹¤
    """
    event_keywords = {
        "íšë“": ["ì–»ë‹¤", "ìŠµë“", "ì†ì— ë„£", "íšë“", "ë°›ë‹¤"],
        "ì†ì‹¤": ["ìƒë‹¤", "ë¹¼ì•—", "ì‚¬ë¼ì§€", "ì—†ì–´ì§€"],
        "ì´ë™": ["ë„ì°©", "ë– ë‚˜", "í–¥í•˜", "ì´ë™", "ì¶œë°œ"],
        "ì „íˆ¬": ["ì‹¸ìš°", "ê³µê²©", "ë°©ì–´", "ì „íˆ¬", "ë§ì„œ"],
        "ê²°ì •": ["ê²°ì‹¬", "ê²°ì •", "ì„ íƒ", "ë§ˆìŒë¨¹"]
    }
    
    event_count = 0
    for category, keywords in event_keywords.items():
        for kw in keywords:
            event_count += text_block.count(kw)
    
    density = event_count / (len(text_block) / 1000)  # per 1000ì
    
    return {
        "density": density,
        "normal_range": (3.0, 12.0),  # Sê¸‰ ê¸°ì¤€
        "judgment": "too_sparse" if density < 3 else "too_dense" if density > 15 else "normal"
    }
```

### 4. ëŒ€í™” ë¹„ìœ¨ ë° í„´ ì†ë„ (Dialogue Ratio & Turn Speed)
```python
def sensor_dialogue_metrics(text_block):
    """
    ëŒ€í™”ëŸ‰ê³¼ ë§ ì£¼ê³ ë°›ëŠ” ì†ë„
    ì¥ë¥´ë§ˆë‹¤ ì •ìƒ ë²”ìœ„ ë‹¤ë¦„
    """
    # ë”°ì˜´í‘œ ê¸°ë°˜ ëŒ€í™” ì¶”ì¶œ
    dialogues = re.findall(r'["\"]([^"\"]+)["\"]', text_block)
    
    dialogue_chars = sum(len(d) for d in dialogues)
    dialogue_ratio = dialogue_chars / len(text_block)
    
    # í„´ ì†ë„: ëŒ€í™” êµ¬ê°„ ì‚¬ì´ ì„œìˆ  í‰ê·  ê¸¸ì´
    narrative_chunks = re.split(r'["\"][^"\"]+["\"]', text_block)
    avg_narrative_length = np.mean([len(c) for c in narrative_chunks if c.strip()])
    
    return {
        "dialogue_ratio": dialogue_ratio,
        "avg_turn_gap": avg_narrative_length,
        "dialogues_per_1k": len(dialogues) / (len(text_block) / 1000)
    }
```

### 5. í›…(Hook) ì‹ í˜¸ ë°€ë„
```python
def sensor_hook_signals(text_block):
    """
    ë¸”ë¡ ëì— 'ë¯¸í•´ê²° ì§ˆë¬¸/ì•½ì†/ìœ„í˜‘/ë³´ìƒ ì˜ˆê³ ' í‘œí˜„
    """
    hook_patterns = {
        "ë¯¸í•´ê²°_ì§ˆë¬¸": ["ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ê³¼ì—°", "ì •ë§", "ì–´ì§¸ì„œ"],
        "ì•½ì†_ì˜ˆê³ ": ["ê³§", "ë¨¸ì§€ì•Šì•„", "ë‹¤ìŒ", "ì´ì œ", "ë“œë””ì–´"],
        "ìœ„í˜‘": ["ìœ„í—˜", "ì¡°ì‹¬", "ê²½ê³ ", "í•¨ì •", "ì "],
        "ë³´ìƒ": ["ë³´ìƒ", "ì–»ê²Œ", "ì†ì— ë„£", "ê¸°íšŒ", "ë¹„ë°€"]
    }
    
    # ë§ˆì§€ë§‰ 200ìì—ì„œ ê²€ìƒ‰
    tail = text_block[-200:]
    
    hook_score = 0
    for category, keywords in hook_patterns.items():
        if any(kw in tail for kw in keywords):
            hook_score += 1
    
    return {
        "hook_score": hook_score,
        "has_strong_hook": hook_score >= 2
    }
```

### 6. ê°ì • ì•„í¬ (Emotion Arc)
```python
def sensor_emotion_arc(blocks):
    """
    ì—°ì† ë¸”ë¡ì˜ ê°ì • ë³€í™” ê³¡ì„ 
    í‰íƒ„í•˜ë©´ ì„í™”, ë³€í™” ìˆìœ¼ë©´ ì •ìƒ
    """
    from transformers import pipeline
    sentiment = pipeline("sentiment-analysis")
    
    emotion_scores = []
    for block in blocks:
        # ë¸”ë¡ì„ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
        sentences = split_sentences(block)
        scores = [sentiment(s)[0]['score'] for s in sentences[:10]]  # ìƒ˜í”Œë§
        emotion_scores.append(np.mean(scores))
    
    # ë³€í™”ëŸ‰ ê³„ì‚°
    variance = np.var(emotion_scores)
    trend = np.polyfit(range(len(emotion_scores)), emotion_scores, 1)[0]
    
    return {
        "variance": variance,
        "trend": trend,
        "shape": "flat" if variance < 0.01 else "dynamic"
    }
```

### 7. ìê¸° ìœ ì‚¬ë„ (Self-Similarity)
```python
def sensor_self_similarity(blocks):
    """
    ë¸”ë¡ ê°„ ì„ë² ë”© ìœ ì‚¬ë„
    ë„ˆë¬´ ë†’ìœ¼ë©´ ë°˜ë³µ/ì„í™”
    """
    embeddings = [get_embedding(block) for block in blocks]
    
    # ì¸ì ‘ ë¸”ë¡ ê°„ ìœ ì‚¬ë„
    similarities = []
    for i in range(len(embeddings) - 1):
        sim = cosine_similarity(embeddings[i], embeddings[i+1])
        similarities.append(sim)
    
    return {
        "avg_similarity": np.mean(similarities),
        "max_similarity": np.max(similarities),
        "stagnation_risk": "high" if np.mean(similarities) > 0.85 else "low"
    }
```

### 8. ë¬¸ì¥ ê¸¸ì´ ë¶„ì‚° (Sentence Length Variance)
```python
def sensor_sentence_variance(text_block):
    """
    ë¬¸ì¥ ê¸¸ì´ì˜ ë³€í™”
    ë¶„ì‚° ë‚®ìœ¼ë©´ ë¦¬ë“¬ ë‹¨ì¡°
    """
    sentences = split_sentences(text_block)
    lengths = [len(s) for s in sentences]
    
    return {
        "mean_length": np.mean(lengths),
        "variance": np.var(lengths),
        "cv": np.std(lengths) / np.mean(lengths),  # ë³€ë™ê³„ìˆ˜
        "rhythm": "monotonous" if np.std(lengths) < 15 else "varied"
    }
```

---

## ğŸ†• ì¶”ì²œ ì¶”ê°€ ì„¼ì„œ (6ê°œ)

### 9. ì‹œê°„ ì••ì¶• ì§€í‘œ (Time Compression Index) â­
```python
def sensor_time_compression(text_block):
    """
    ë‹¨ìœ„ ì‹œê°„ë‹¹ ì‚¬ê±´ ë³µì¡ë„
    "1ì‹œê°„ ë§Œì— 100km ì´ë™" ê°™ì€ ë¹„í˜„ì‹¤ ê°ì§€
    """
    import re
    
    # ì‹œê°„ í‘œí˜„ ì¶”ì¶œ
    time_patterns = [
        (r'(\d+)\s*ì‹œê°„', 'hour'),
        (r'(\d+)\s*ë¶„', 'minute'),
        (r'(\d+)\s*ì¼', 'day'),
        (r'(\d+)\s*ë‹¬', 'month')
    ]
    
    time_units = []
    for pattern, unit in time_patterns:
        matches = re.findall(pattern, text_block)
        for m in matches:
            time_units.append((int(m), unit))
    
    # ì‚¬ê±´ ë³µì¡ë„ (ìƒíƒœ ë³€í™” + ì´ë™ + ëŒ€í™”)
    event_complexity = (
        sensor_state_change_density(text_block)['density'] +
        count_location_changes(text_block) +
        sensor_dialogue_metrics(text_block)['dialogues_per_1k']
    )
    
    # ì••ì¶• ë¹„ìœ¨
    if time_units:
        min_time_hours = min(convert_to_hours(t) for t in time_units)
        compression_ratio = event_complexity / max(min_time_hours, 0.1)
    else:
        compression_ratio = 0
    
    return {
        "compression_ratio": compression_ratio,
        "judgment": "excessive" if compression_ratio > 50 else "normal"
    }
```

### 10. ì§€ì‹ ìˆ˜ì¤€ ì í”„ ê°ì§€ (Knowledge Level Jump) â­
```python
def sensor_knowledge_jump(blocks, character_profile):
    """
    ìºë¦­í„°ê°€ 'ì•Œ ìˆ˜ ì—†ëŠ”' ì§€ì‹ì„ ê°‘ìê¸° ì‚¬ìš©
    ì˜ˆ: ë¬¸ë§¹ì´ ê°‘ìê¸° ê³ ëŒ€ì–´ í•´ë…
    """
    # ì§€ì‹ ê´€ë ¨ í‚¤ì›Œë“œ ë°€ë„
    knowledge_indicators = {
        "í•™ìˆ ": ["ì´ë¡ ", "ì›ë¦¬", "ë²•ì¹™", "ì²´ê³„", "í•™ë¬¸"],
        "ì „ë¬¸ê¸°ìˆ ": ["ê¸°ìˆ ", "ì œì‘", "ì„¤ê³„", "ê³µì •"],
        "ê³ ê¸‰ì–¸ì–´": ["ê³ ëŒ€", "ë¬¸ì", "í•´ë…", "ë²ˆì—­", "ë¹„ë¬¸"],
        "ì „ëµ": ["ì „ëµ", "ì „ìˆ ", "í¬ì„", "ê³„ëµ"]
    }
    
    knowledge_scores = []
    for block in blocks:
        score = 0
        for category, keywords in knowledge_indicators.items():
            score += sum(block.count(kw) for kw in keywords)
        knowledge_scores.append(score / (len(block) / 1000))
    
    # ê¸‰ì¦ ê°ì§€
    jumps = []
    for i in range(len(knowledge_scores) - 1):
        if knowledge_scores[i+1] > knowledge_scores[i] * 3:  # 3ë°° ì´ìƒ ì¦ê°€
            jumps.append({
                "location": i+1,
                "before": knowledge_scores[i],
                "after": knowledge_scores[i+1]
            })
    
    return {
        "knowledge_trend": knowledge_scores,
        "sudden_jumps": jumps,
        "risk": "high" if jumps else "low"
    }
```

### 11. ìì› ì¼ê´€ì„± (Resource Consistency) â­
```python
def sensor_resource_consistency(text_blocks):
    """
    ëˆ/ë§ˆë‚˜/ì²´ë ¥ ê°™ì€ ìì›ì˜ ì¦ê° ì¶”ì 
    'ëˆ ì—†ë‹¤' í–ˆë‹¤ê°€ ê°‘ìê¸° 'ê±°ê¸ˆ ì§€ë¶ˆ'ì€ ì˜¤ë¥˜
    """
    resource_keywords = {
        "money": {
            "positive": ["ê¸ˆí™”", "ì€í™”", "ë¶€ì", "ì¬ì‚°", "ëˆ"],
            "negative": ["ê°€ë‚œ", "ë¹ˆê³¤", "ëˆ ì—†", "ê¶í•"]
        },
        "health": {
            "positive": ["ê±´ê°•", "íšŒë³µ", "ì¹˜ìœ ", "ì™„ì¾Œ"],
            "negative": ["ë¶€ìƒ", "ì¤‘ìƒ", "í”¼", "ìƒì²˜"]
        },
        "mana": {
            "positive": ["ë§ˆë‚˜", "ë§ˆë ¥", "ê¸°ë ¥", "ì¶©ë§Œ"],
            "negative": ["ê³ ê°ˆ", "ë°”ë‹¥", "ì†Œì§„", "ì—†"]
        }
    }
    
    resource_states = []
    for block in text_blocks:
        state = {}
        for resource, indicators in resource_keywords.items():
            pos_count = sum(block.count(kw) for kw in indicators['positive'])
            neg_count = sum(block.count(kw) for kw in indicators['negative'])
            state[resource] = pos_count - neg_count  # ì–‘ìˆ˜ë©´ í’ì¡±, ìŒìˆ˜ë©´ ë¶€ì¡±
        resource_states.append(state)
    
    # ëª¨ìˆœ ê°ì§€
    contradictions = []
    for i in range(len(resource_states) - 1):
        for resource in resource_keywords.keys():
            if (resource_states[i][resource] < -2 and  # ë§¤ìš° ë¶€ì¡±
                resource_states[i+1][resource] > 2):   # ê°‘ìê¸° í’ì¡±
                contradictions.append({
                    "resource": resource,
                    "location": i+1,
                    "before": resource_states[i][resource],
                    "after": resource_states[i+1][resource]
                })
    
    return {
        "resource_trend": resource_states,
        "contradictions": contradictions
    }
```

### 12. ê³µê°„ ì¼ê´€ì„± (Spatial Consistency) â­
```python
def sensor_spatial_consistency(text_blocks):
    """
    ê³µê°„/ê±°ë¦¬ í‘œí˜„ì˜ ì¼ê´€ì„±
    "ì¢ì€ ë°©"ì— "ìˆ˜ì‹­ ëª…" ê°™ì€ ëª¨ìˆœ
    """
    # ê³µê°„ í¬ê¸° í‘œí˜„
    space_size = {
        "small": ["ì¢ì€", "ì‘ì€", "ë¹„ì¢", "í˜‘ì†Œ"],
        "large": ["ë„“ì€", "ê´‘í™œ", "ê±°ëŒ€", "ë“œë„“"]
    }
    
    # ì¸ì›ìˆ˜ í‘œí˜„
    crowd_size_pattern = r'(\d+)\s*ëª…|ìˆ˜ì‹­|ìˆ˜ë°±|ë¬´ë¦¬'
    
    inconsistencies = []
    for i, block in enumerate(text_blocks):
        # ê³µê°„ í¬ê¸° íŒì •
        small_count = sum(block.count(kw) for kw in space_size['small'])
        large_count = sum(block.count(kw) for kw in space_size['large'])
        
        # ì¸ì›ìˆ˜ ì¶”ì¶œ
        crowd_matches = re.findall(r'(\d+)\s*ëª…', block)
        crowds = [int(m) for m in crowd_matches]
        
        # ëª¨ìˆœ ì²´í¬
        if small_count > large_count and crowds and max(crowds) > 10:
            inconsistencies.append({
                "block": i,
                "issue": f"Small space but {max(crowds)} people",
                "severity": max(crowds) / 10
            })
    
    return {
        "inconsistencies": inconsistencies,
        "risk": "high" if inconsistencies else "low"
    }
```

### 13. ê³„ì ˆ/ë‚ ì”¨ ì¼ê´€ì„± (Seasonal Consistency)
```python
def sensor_seasonal_consistency(text_blocks):
    """
    ê³„ì ˆê³¼ ë‚ ì”¨/ì‹ë¬¼ í‘œí˜„ì˜ ì¼ê´€ì„±
    "í•œê²¨ìš¸"ì— "ë²šê½ƒ" ê°™ì€ ëª¨ìˆœ
    """
    season_keywords = {
        "ë´„": ["ë´„", "ë”°ëœ»", "ë²šê½ƒ", "ê½ƒ", "ìƒˆì‹¹"],
        "ì—¬ë¦„": ["ì—¬ë¦„", "ë”ìœ„", "ë¬´ë”ìœ„", "ì—´ê¸°"],
        "ê°€ì„": ["ê°€ì„", "ë‚™ì—½", "ë‹¨í’", "ì„œëŠ˜"],
        "ê²¨ìš¸": ["ê²¨ìš¸", "ëˆˆ", "ì¶”ìœ„", "í•œíŒŒ", "ì–¼ìŒ"]
    }
    
    contradictions = []
    for i, block in enumerate(text_blocks):
        season_scores = {s: sum(block.count(kw) for kw in kws) 
                        for s, kws in season_keywords.items()}
        
        dominant = max(season_scores, key=season_scores.get)
        
        # ë°˜ëŒ€ ê³„ì ˆ ìš”ì†Œ ì²´í¬
        opposite = {"ë´„": "ê°€ì„", "ì—¬ë¦„": "ê²¨ìš¸", "ê°€ì„": "ë´„", "ê²¨ìš¸": "ì—¬ë¦„"}
        if season_scores[opposite.get(dominant, "")] > season_scores[dominant] * 0.5:
            contradictions.append({
                "block": i,
                "dominant": dominant,
                "conflicting": opposite[dominant],
                "severity": season_scores[opposite[dominant]]
            })
    
    return {
        "contradictions": contradictions
    }
```

### 14. ì¸ê³¼ ì²´ì¸ ë°€ë„ (Causal Chain Density) â­â­
```python
def sensor_causal_chain(text_blocks):
    """
    ì›ì¸-ê²°ê³¼ ì—°ê²° ë°€ë„
    'ì™œëƒí•˜ë©´', 'ê·¸ë˜ì„œ', 'ë•Œë¬¸ì—' ê°™ì€ ë…¼ë¦¬ ì—°ê²°ì–´
    """
    causal_markers = {
        "ì›ì¸": ["ì™œëƒí•˜ë©´", "ë•Œë¬¸ì—", "íƒ“ì—", "ì´ìœ ë¡œ", "ì¸í•´"],
        "ê²°ê³¼": ["ê·¸ë˜ì„œ", "ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê²°êµ­", "ê·¸ ê²°ê³¼"],
        "ì¡°ê±´": ["ë§Œì•½", "ê²½ìš°", "~ë‹¤ë©´", "~ë¼ë©´"],
        "ëŒ€ì¡°": ["í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ë°˜ë©´", "ì˜¤íˆë ¤"]
    }
    
    chain_densities = []
    for block in text_blocks:
        total_markers = 0
        for category, markers in causal_markers.items():
            total_markers += sum(block.count(m) for m in markers)
        
        density = total_markers / (len(block) / 1000)
        chain_densities.append(density)
    
    return {
        "avg_density": np.mean(chain_densities),
        "min_density": np.min(chain_densities),
        "weak_blocks": [i for i, d in enumerate(chain_densities) if d < 2.0],
        "judgment": "weak_logic" if np.mean(chain_densities) < 2.5 else "normal"
    }
```

---

# PART 2: ìˆ˜ë°± ê¶Œ ì†Œì„¤ì—ì„œ ì •ìƒ ë¶„í¬ ìƒì„±

## Step 1: ëŒ€ëŸ‰ ì¸¡ì • íŒŒì´í”„ë¼ì¸

```python
import json
from pathlib import Path
from tqdm import tqdm

class NovelCorpusAnalyzer:
    def __init__(self, novels_dir, block_size=1200):
        self.novels_dir = Path(novels_dir)
        self.block_size = block_size
        self.sensors = [
            sensor_lexical_diversity,
            sensor_summary_change,
            sensor_state_change_density,
            sensor_dialogue_metrics,
            sensor_hook_signals,
            sensor_emotion_arc,
            sensor_self_similarity,
            sensor_sentence_variance,
            sensor_time_compression,
            sensor_knowledge_jump,
            sensor_resource_consistency,
            sensor_spatial_consistency,
            sensor_seasonal_consistency,
            sensor_causal_chain
        ]
    
    def process_all_novels(self, grade_filter=None):
        """
        ëª¨ë“  ì†Œì„¤ ì²˜ë¦¬
        grade_filter: 'S', 'A', 'B', 'C' ì¤‘ í•˜ë‚˜ (ë“±ê¸‰ë³„ ë¶„ë¦¬ìš©)
        """
        results = {
            "metadata": [],
            "measurements": []
        }
        
        novel_files = list(self.novels_dir.glob("*.txt"))
        
        for novel_path in tqdm(novel_files, desc="Processing novels"):
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ ë“±ê¸‰ ì •ë³´ ë“±)
            metadata = self.extract_metadata(novel_path)
            
            if grade_filter and metadata.get('grade') != grade_filter:
                continue
            
            # ì†Œì„¤ ì½ê¸°
            with open(novel_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # ë¸”ë¡ìœ¼ë¡œ ë¶„í• 
            blocks = self.split_into_blocks(text)
            
            # ê° ë¸”ë¡ ì¸¡ì •
            measurements = self.measure_blocks(blocks, metadata)
            
            results["metadata"].append(metadata)
            results["measurements"].extend(measurements)
        
        return results
    
    def split_into_blocks(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ê³ ì • í¬ê¸° ë¸”ë¡ìœ¼ë¡œ ë¶„í• """
        blocks = []
        for i in range(0, len(text), self.block_size):
            block = text[i:i + self.block_size]
            if len(block) > 500:  # ë„ˆë¬´ ì§§ì€ ë¸”ë¡ ì œì™¸
                blocks.append(block)
        return blocks
    
    def measure_blocks(self, blocks, metadata):
        """ê° ë¸”ë¡ì— 14ê°œ ì„¼ì„œ ì ìš©"""
        measurements = []
        
        for i, block in enumerate(blocks):
            measurement = {
                "novel_id": metadata['id'],
                "block_index": i,
                "features": {}
            }
            
            # ì„¼ì„œ 1-8: ë‹¨ì¼ ë¸”ë¡ ì¸¡ì •
            for sensor in self.sensors[:8]:
                try:
                    result = sensor(block)
                    measurement["features"][sensor.__name__] = result
                except Exception as e:
                    print(f"Sensor {sensor.__name__} failed: {e}")
            
            # ì„¼ì„œ 9-14: ì»¨í…ìŠ¤íŠ¸ í•„ìš” (ì´ì „/ë‹¤ìŒ ë¸”ë¡)
            if i > 0:
                # ì‹œê°„ ì••ì¶•, ì§€ì‹ ì í”„ ë“±ì€ ì—°ì† ë¸”ë¡ í•„ìš”
                context_sensors = self.sensors[8:]
                window = blocks[max(0, i-2):min(len(blocks), i+3)]
                
                for sensor in context_sensors:
                    try:
                        result = sensor(window)
                        measurement["features"][sensor.__name__] = result
                    except Exception as e:
                        print(f"Context sensor {sensor.__name__} failed: {e}")
            
            measurements.append(measurement)
        
        return measurements
    
    def extract_metadata(self, novel_path):
        """íŒŒì¼ëª…ì´ë‚˜ ë©”íƒ€ íŒŒì¼ì—ì„œ ë“±ê¸‰ ì •ë³´ ì¶”ì¶œ"""
        # ì˜ˆ: "novel_s_001.txt" -> grade='S'
        filename = novel_path.stem
        
        grade = None
        if '_s_' in filename.lower():
            grade = 'S'
        elif '_a_' in filename.lower():
            grade = 'A'
        elif '_b_' in filename.lower():
            grade = 'B'
        elif '_c_' in filename.lower():
            grade = 'C'
        
        return {
            "id": filename,
            "path": str(novel_path),
            "grade": grade,
            "size": novel_path.stat().st_size
        }
```

---

## Step 2: ë¶„í¬ í†µê³„ ìƒì„±

```python
import pandas as pd
import numpy as np
from scipy import stats

class DistributionBuilder:
    def __init__(self, measurements):
        self.measurements = measurements
        self.df = pd.DataFrame([
            {
                "novel_id": m["novel_id"],
                "block_index": m["block_index"],
                **self.flatten_features(m["features"])
            }
            for m in measurements
        ])
    
    def flatten_features(self, features):
        """ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ë¥¼ í‰íƒ„í™”"""
        flat = {}
        for sensor_name, sensor_result in features.items():
            if isinstance(sensor_result, dict):
                for key, value in sensor_result.items():
                    if isinstance(value, (int, float)):
                        flat[f"{sensor_name}_{key}"] = value
            elif isinstance(sensor_result, (int, float)):
                flat[sensor_name] = sensor_result
        return flat
    
    def build_distributions(self, grade='S'):
        """íŠ¹ì • ë“±ê¸‰ì˜ ë¶„í¬ ìƒì„±"""
        grade_data = self.df[self.df['novel_id'].str.contains(f'_{grade.lower()}_')]
        
        distributions = {}
        
        for column in grade_data.select_dtypes(include=[np.number]).columns:
            if column not in ['block_index']:
                values = grade_data[column].dropna()
                
                if len(values) > 30:  # ì¶©ë¶„í•œ ìƒ˜í”Œ
                    distributions[column] = {
                        "mean": float(values.mean()),
                        "std": float(values.std()),
                        "median": float(values.median()),
                        "q25": float(values.quantile(0.25)),
                        "q75": float(values.quantile(0.75)),
                        "min": float(values.min()),
                        "max": float(values.max()),
                        "n_samples": len(values)
                    }
        
        return distributions
    
    def save_distributions(self, output_path):
        """ë“±ê¸‰ë³„ ë¶„í¬ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        all_distributions = {}
        
        for grade in ['S', 'A', 'B', 'C']:
            all_distributions[grade] = self.build_distributions(grade)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_distributions, f, indent=2, ensure_ascii=False)
        
        print(f"Distributions saved to {output_path}")
        return all_distributions
```

---

## Step 3: ì´ìƒ íƒì§€ ì—”ì§„

```python
class AnomalyDetector:
    def __init__(self, distributions_path):
        """ì €ì¥ëœ ë¶„í¬ ë¡œë“œ"""
        with open(distributions_path, 'r', encoding='utf-8') as f:
            self.distributions = json.load(f)
    
    def detect_anomalies(self, text_blocks, reference_grade='S'):
        """
        ìƒˆ í…ìŠ¤íŠ¸ ë¸”ë¡ë“¤ì˜ ì´ìƒ íƒì§€
        """
        # ê°™ì€ ì„¼ì„œë¡œ ì¸¡ì •
        analyzer = NovelCorpusAnalyzer(".")
        measurements = analyzer.measure_blocks(text_blocks, {"id": "test"})
        
        anomalies = []
        ref_dist = self.distributions[reference_grade]
        
        for i, measurement in enumerate(measurements):
            block_anomalies = []
            
            for feature_name, value in measurement["features"].items():
                if isinstance(value, dict):
                    # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ ì²˜ë¦¬
                    for sub_key, sub_value in value.items():
                        if isinstance(sub_value, (int, float)):
                            full_key = f"{feature_name}_{sub_key}"
                            anomaly = self.check_feature(
                                full_key, sub_value, ref_dist
                            )
                            if anomaly:
                                block_anomalies.append(anomaly)
                elif isinstance(value, (int, float)):
                    anomaly = self.check_feature(feature_name, value, ref_dist)
                    if anomaly:
                        block_anomalies.append(anomaly)
            
            if block_anomalies:
                anomalies.append({
                    "block_index": i,
                    "anomalies": block_anomalies,
                    "severity": self.calculate_severity(block_anomalies)
                })
        
        return anomalies
    
    def check_feature(self, feature_name, value, ref_dist):
        """ê°œë³„ íŠ¹ì§•ì´ ì •ìƒ ë²”ìœ„ì¸ì§€ í™•ì¸"""
        if feature_name not in ref_dist:
            return None  # ë¶„í¬ ì •ë³´ ì—†ìŒ
        
        dist = ref_dist[feature_name]
        mean = dist['mean']
        std = dist['std']
        
        # Z-score ê³„ì‚°
        if std > 0:
            z_score = abs((value - mean) / std)
        else:
            z_score = 0
        
        # 3-sigma ë£°: 99.7% ì‹ ë¢°êµ¬ê°„
        if z_score > 3:
            return {
                "feature": feature_name,
                "value": value,
                "expected_mean": mean,
                "expected_std": std,
                "z_score": z_score,
                "deviation": "extreme"
            }
        elif z_score > 2:
            return {
                "feature": feature_name,
                "value": value,
                "expected_mean": mean,
                "expected_std": std,
                "z_score": z_score,
                "deviation": "moderate"
            }
        
        return None
    
    def calculate_severity(self, block_anomalies):
        """ë¸”ë¡ì˜ ì „ì²´ ì‹¬ê°ë„ ê³„ì‚°"""
        if not block_anomalies:
            return 0.0
        
        severity = 0.0
        for anomaly in block_anomalies:
            if anomaly['deviation'] == 'extreme':
                severity += anomaly['z_score'] * 0.5
            else:
                severity += anomaly['z_score'] * 0.2
        
        return min(severity, 10.0)  # ìµœëŒ€ 10ì 
```

---

# PART 3: ì‹¤ì „ ì‚¬ìš© ì˜ˆì‹œ

```python
def main():
    # ===== 1ë‹¨ê³„: ì½”í¼ìŠ¤ ë¶„ì„ (í•œ ë²ˆë§Œ ì‹¤í–‰) =====
    print("Step 1: Analyzing novel corpus...")
    
    analyzer = NovelCorpusAnalyzer(
        novels_dir="./novels",  # ìˆ˜ë°± ê¶Œ ì†Œì„¤ í´ë”
        block_size=1200
    )
    
    # ë“±ê¸‰ë³„ë¡œ ë¶„ë¦¬ ì²˜ë¦¬
    all_measurements = []
    for grade in ['S', 'A', 'B', 'C']:
        print(f"Processing {grade}-grade novels...")
        results = analyzer.process_all_novels(grade_filter=grade)
        all_measurements.extend(results["measurements"])
    
    # ===== 2ë‹¨ê³„: ë¶„í¬ ìƒì„± =====
    print("\nStep 2: Building distributions...")
    
    builder =

ì´ê±´ í´ë¡œë“œì˜ ë°©ì‹.



ì•„ë˜ëŠ” ë”¥ì‹œí¬ë°©ì‹

# ğŸ“Š ì¼ë°˜ ì‹¤ìˆ˜/ê°œì—°ì„± ì˜¤ë¥˜ 14ê°œ ì„¼ì„œ ìˆ˜ì¹˜í™” ì•Œê³ ë¦¬ì¦˜ ë° í™•ì¥ ë°©ì•ˆ

## ğŸ” **14ê°œ í•µì‹¬ ì˜¤ë¥˜ ì„¼ì„œ ìˆ˜ì¹˜í™” ë°©ë²•**

### **1. ì‹œê°„ ì••ì¶• ê³¼ë‹¤ (ë¹„í˜„ì‹¤ì  ì‹œê°„ ë‹¨ì¶•)**
```python
def time_compression_score(text_chunk):
    # ì‹œê°„ í‘œí˜„ ì¶”ì¶œ (ì •ê·œí‘œí˜„ì‹)
    time_expressions = extract_time_phrases(text_chunk)
    
    # ì‹œê°„ ë‹¨ìœ„ ì •ê·œí™” (ë¶„ â†’ ì‹œê°„, ì‹œê°„ â†’ ì¼ ë“±)
    normalized_times = normalize_time_units(time_expressions)
    
    # ì¼ê´€ì„± ê³„ì‚°
    if len(normalized_times) < 2:
        return 0
    
    # ì‹œê°„ íë¦„ ì†ë„ ê³„ì‚° (ì‚¬ê±´ ìˆ˜ / ì†Œìš” ì‹œê°„)
    events_count = count_events(text_chunk)  # ìƒíƒœ ë³€í™” ë™ì‚¬ ì¹´ìš´íŠ¸
    total_time_hours = sum(normalized_times)
    
    if total_time_hours == 0:
        return 0
    
    events_per_hour = events_count / total_time_hours
    
    # ê¸°ì¤€ì„ : ì¼ë°˜ ì†Œì„¤ì€ ì‹œê°„ë‹¹ 0.5~2ê°œ ì‚¬ê±´
    # ë¹„í˜„ì‹¤ì : ì‹œê°„ë‹¹ 10ê°œ ì´ìƒ ì‚¬ê±´
    return min(events_per_hour / 10, 1.0)
```

### **2. ìºë¦­í„° ê¸°ì–µë ¥ í¸ì°¨ (ì•ë’¤ ê¸°ì–µ ë¶ˆì¼ì¹˜)**
```python
def memory_consistency_score(text_chunk, previous_chunks):
    # ëª…ì‚¬êµ¬ ì¶”ì¶œ (ì§€ì‹/ì‚¬ì‹¤ ì–¸ê¸‰)
    facts = extract_factual_statements(text_chunk)
    
    # ì´ì „ ì²­í¬ì—ì„œ ë™ì¼ ê°œì²´ì— ëŒ€í•œ ì–¸ê¸‰ ì°¾ê¸°
    inconsistencies = []
    for fact in facts:
        entity, attribute, value = fact
        previous_values = find_previous_values(entity, attribute, previous_chunks)
        
        if previous_values and value not in previous_values:
            # ë¶ˆì¼ì¹˜ ë°œê²¬
            confidence = calculate_confidence(fact, previous_values)
            inconsistencies.append(confidence)
    
    return len(inconsistencies) / max(len(facts), 1)
```

### **3. ìì› ë§ê° (ë§ˆë‚˜/ì²´ë ¥/ëˆ ìˆ˜ì¹˜ ë¶ˆì¼ì¹˜)**
```python
def resource_consistency_score(text_chunk, resource_log):
    # ìì› ê´€ë ¨ ë™ì‚¬/ìˆ˜ì¹˜ ì¶”ì¶œ
    resources = extract_resources(text_chunk)
    
    # ë¡œê·¸ì™€ ë¹„êµ
    discrepancies = []
    for resource in resources:
        name, change, current = resource
        expected = resource_log.get(name, {}).get('current', 0)
        
        # ìˆ˜ì¹˜ ë¶ˆì¼ì¹˜ ê³„ì‚°
        if abs(current - expected) > expected * 0.1:  # 10% ì´ìƒ ì°¨ì´
            discrepancy = abs(current - expected) / max(expected, 1)
            discrepancies.append(min(discrepancy, 1.0))
    
    return sum(discrepancies) / max(len(discrepancies), 1)
```

### **4. ê³µê°„ì  ë¶ˆì¼ì¹˜ (ë°© í¬ê¸° vs ì¸ì› ë¹„ìœ¨)**
```python
def spatial_consistency_score(text_chunk):
    # ê³µê°„/í¬ê¸° í‘œí˜„ ì¶”ì¶œ
    spaces = extract_spatial_descriptions(text_chunk)
    
    # ë¬¼ë¦¬ì  ê°€ëŠ¥ì„± ê³„ì‚°
    impossibilities = []
    for space in spaces:
        # ì˜ˆ: "ì‘ì€ ë°©ì— 100ëª…ì´ ë“¤ì–´ê°”ë‹¤"
        capacity = calculate_capacity(space['dimensions'], space['type'])
        occupants = count_occupants(text_chunk, space['name'])
        
        if occupants > capacity * 1.5:  # 150% ì´ˆê³¼
            ratio = occupants / capacity
            impossibilities.append(min((ratio - 1.5) / 3, 1.0))
    
    return sum(impossibilities) / max(len(impossibilities), 1)
```

### **5. ê°ì • ì™€í”Œë˜ì‹œ (ë„ˆë¬´ ë¹ ë¥¸ ê°ì • ì „í™˜)**
```python
def emotional_whiplash_score(text_chunk):
    # ê°ì • ì¶”ì¶œ (ê°ì • ì‚¬ì „ + ë¬¸ë§¥)
    emotions = extract_emotions_by_sentence(text_chunk)
    
    if len(emotions) < 2:
        return 0
    
    # ê°ì • ë³€í™” ê°•ë„ ê³„ì‚°
    changes = []
    for i in range(1, len(emotions)):
        prev_emo = emotions[i-1]
        curr_emo = emotions[i]
        
        # ê°ì • ë²¡í„° ê±°ë¦¬ ê³„ì‚° (ê¸ì •-ë¶€ì • ì¶•, ê°ì„±ë„ ì¶•)
        distance = emotional_distance(prev_emo, curr_emo)
        
        # ë¬¸ì¥ ê°„ ê°ì • ë³€í™”ê°€ ì§€ë‚˜ì¹˜ê²Œ ê¸‰ê²©í•œê°€?
        if distance > EMOTION_CHANGE_THRESHOLD:
            changes.append(min(distance / MAX_DISTANCE, 1.0))
    
    return sum(changes) / max(len(emotions) - 1, 1)
```

### **6. ì„¤ì • ë§ê° (ì´ˆë°˜ ì„¤ì •ê³¼ í›„ë°˜ ì„¤ì • ì¶©ëŒ)**
```python
def setting_consistency_score(current_chunk, setting_baseline):
    # ì„¤ì • ìš”ì†Œ ì¶”ì¶œ (ì‹œëŒ€, ê¸°ìˆ  ìˆ˜ì¤€, ë§ˆë²• ê·œì¹™ ë“±)
    current_settings = extract_settings(current_chunk)
    
    # ê¸°ì¤€ ì„¤ì •ê³¼ ë¹„êµ
    conflicts = []
    for key, value in current_settings.items():
        if key in setting_baseline:
            baseline_value = setting_baseline[key]
            
            # ì¶©ëŒ ê²€ì¶œ (ì˜ˆ: ì´ˆë°˜ "ë§ˆë²• ì—†ìŒ" vs í›„ë°˜ "ë§ˆë²• ì‚¬ìš©")
            if not is_compatible(value, baseline_value):
                confidence = calculate_conflict_confidence(key, value, baseline_value)
                conflicts.append(confidence)
    
    return len(conflicts) / max(len(current_settings), 1)
```

### **7. ê³„ì ˆì  ì˜¤ë¥˜ (ê²¨ìš¸ì— ë´„ ê½ƒ)**
```python
def seasonal_consistency_score(text_chunk, timeline):
    # ê³„ì ˆ/ê¸°í›„ í‘œí˜„ ì¶”ì¶œ
    seasonal_indicators = extract_seasonal_indicators(text_chunk)
    
    # íƒ€ì„ë¼ì¸ì—ì„œ í˜„ì¬ ê³„ì ˆ í™•ì¸
    current_season = timeline.get_current_season()
    
    errors = []
    for indicator in seasonal_indicators:
        # ê³„ì ˆê³¼ ë§ì§€ ì•ŠëŠ” í˜„ìƒ ê²€ì¶œ
        if not is_seasonally_appropriate(indicator, current_season):
            # ì˜¤ë¥˜ ì‹¬ê°ë„ ê³„ì‚°
            severity = calculate_seasonal_error_severity(indicator, current_season)
            errors.append(severity)
    
    return sum(errors) / max(len(seasonal_indicators), 1)
```

### **8. ì˜í•™ì  ë¶€ì •í™•ì„± (í˜„ì‹¤ì ì´ì§€ ì•Šì€ ë¶€ìƒ/ì¹˜ìœ )**
```python
def medical_realism_score(text_chunk):
    # ì˜í•™ì  ìƒí™© ì¶”ì¶œ
    medical_events = extract_medical_events(text_chunk)
    
    realism_scores = []
    for event in medical_events:
        # ì˜í•™ ì§€ì‹ ë² ì´ìŠ¤ì™€ ë¹„êµ
        plausibility = check_medical_plausibility(
            event['injury'],
            event['treatment'],
            event['recovery_time']
        )
        
        realism_scores.append(1 - plausibility)  # ë¶€ì •í™•ì„± ì ìˆ˜
    
    return sum(realism_scores) / max(len(realism_scores), 1)
```

### **9. ê¸°ìˆ  ì‹œëŒ€ì°©ì˜¤ (ì‹œëŒ€ì— ë§ì§€ ì•ŠëŠ” ë„êµ¬)**
```python
def technological_anachronism_score(text_chunk, era_definition):
    # ê¸°ìˆ /ë„êµ¬ ì–¸ê¸‰ ì¶”ì¶œ
    technologies = extract_technologies(text_chunk)
    
    anachronisms = []
    for tech in technologies:
        # ì‹œëŒ€ ì •ì˜ì™€ ë¹„êµ
        if not is_era_appropriate(tech, era_definition):
            # ì‹œëŒ€ ì°¨ì´ ê³„ì‚° (ì—°ë„ ì°¨ì´)
            year_gap = calculate_year_gap(tech['invention_year'], era_definition['year'])
            severity = min(year_gap / 100, 1.0)  # 100ë…„ ì°¨ì´ = ìµœëŒ€ ì ìˆ˜
            anachronisms.append(severity)
    
    return sum(anachronisms) / max(len(technologies), 1)
```

### **10. ë¬¼ë¦¬ ë²•ì¹™ ë¬´ì‹œ (ì¸ê°„ í•œê³„ ì´ˆê³¼ í–‰ë™)**
```python
def physics_violation_score(text_chunk):
    # ë¬¼ë¦¬ì  í–‰ë™ ì¶”ì¶œ
    physical_actions = extract_physical_actions(text_chunk)
    
    violations = []
    for action in physical_actions:
        # ë¬¼ë¦¬ì  ê°€ëŠ¥ì„± í‰ê°€
        possibility = calculate_physical_possibility(
            action['type'],
            action['magnitude'],
            action['actor_capability']
        )
        
        if possibility < PHYSICAL_POSSIBILITY_THRESHOLD:
            violations.append(1 - possibility)
    
    return sum(violations) / max(len(physical_actions), 1)
```

### **11. ë¬¸í™”ì  ê³ ì •ê´€ë… (í”¼ìƒì ì´ê³  í‹€ì— ë°•íŒ ë¬˜ì‚¬)**
```python
def cultural_stereotype_score(text_chunk):
    # ë¬¸í™”/ì§‘ë‹¨ ë¬˜ì‚¬ ì¶”ì¶œ
    cultural_descriptions = extract_cultural_descriptions(text_chunk)
    
    stereotype_scores = []
    for desc in cultural_descriptions:
        # ê³ ì •ê´€ê²¬ ë°ì´í„°ë² ì´ìŠ¤ì™€ ë¹„êµ
        stereotype_match = check_stereotype_database(
            desc['group'],
            desc['traits'],
            desc['context']
        )
        
        stereotype_scores.append(stereotype_match)
    
    return sum(stereotype_scores) / max(len(cultural_descriptions), 1)
```

### **12. ìºë¦­í„° ë™ê¸° ë¶ˆì¼ì¹˜ (ë§ê³¼ í–‰ë™ ë¶ˆì¼ì¹˜)**
```python
def character_motivation_consistency(chunk):
    # ìºë¦­í„° ì„ ì–¸ì  ì˜ë„ ì¶”ì¶œ
    stated_intentions = extract_stated_intentions(chunk)
    
    # ì‹¤ì œ í–‰ë™ ì¶”ì¶œ
    actual_actions = extract_actions_by_character(chunk)
    
    mismatches = []
    for char, intentions in stated_intentions.items():
        if char in actual_actions:
            for intention in intentions:
                # ì˜ë„ì™€ í–‰ë™ ì¼ì¹˜ë„ ê³„ì‚°
                alignment = calculate_intention_action_alignment(
                    intention,
                    actual_actions[char]
                )
                
                if alignment < ALIGNMENT_THRESHOLD:
                    mismatches.append(1 - alignment)
    
    return sum(mismatches) / max(len(mismatches), 1)
```

### **13. ëŒ€í™” ìì—°ìŠ¤ëŸ¬ì›€ (ì¸ê°„ ëŒ€í™” íŒ¨í„´ ìœ„ë°˜)**
```python
def dialogue_naturalness_score(dialogue_chunk):
    # ëŒ€í™” ì¶”ì¶œ
    dialogues = extract_dialogues(dialogue_chunk)
    
    if len(dialogues) < 2:
        return 0
    
    unnaturalness_scores = []
    for i in range(len(dialogues)-1):
        turn1 = dialogues[i]
        turn2 = dialogues[i+1]
        
        # ëŒ€í™” í„´ ìì—°ìŠ¤ëŸ¬ì›€ ì²´í¬
        score = calculate_turn_transition_naturalness(turn1, turn2)
        unnaturalness_scores.append(1 - score)
    
    return sum(unnaturalness_scores) / max(len(unnaturalness_scores), 1)
```

### **14. ì„œì‚¬ ì•½ì† ìœ„ë°˜ (ë³µì„  íšŒìˆ˜ ì‹¤íŒ¨)**
```python
def narrative_promise_violation_score(current_chunk, setup_chekhovs_gun):
    # ì´ì „ì— ì„¤ì •ëœ ë³µì„ /ì•½ì†
    promises = setup_chekhovs_gun
    
    # í˜„ì¬ ì²­í¬ì—ì„œ í•´ê²°/ì–¸ê¸‰ëœ ìš”ì†Œ
    resolutions = extract_resolutions(current_chunk)
    
    violations = []
    for promise in promises:
        # ì•½ì† ìœ íš¨ ê¸°ê°„ ì²´í¬
        if is_promise_expired(promise, current_chunk.position):
            # í•´ê²°ë˜ì—ˆëŠ”ê°€?
            if not is_resolved(promise, resolutions):
                # ìœ„ë°˜ ì‹¬ê°ë„ ê³„ì‚°
                severity = calculate_promise_importance(promise)
                violations.append(severity)
    
    return sum(violations) / max(len(promises), 1)
```

## ğŸ†• **ì¶”ê°€ 6ê°œ í•µì‹¬ ì„¼ì„œ (ë‹ˆê°€ ì¶”ì²œ)**

### **15. ì‚¬íšŒì  íˆì—ë¼ë¥´í‚¤ ìœ„ë°˜ (ì‹ ë¶„/ê³„ê¸‰ ë…¼ë¦¬ ë¶•ê´´)**
```python
def social_hierarchy_violation_score(chunk, world_rules):
    # ì‚¬íšŒì  ìƒí˜¸ì‘ìš© ì¶”ì¶œ
    interactions = extract_social_interactions(chunk)
    
    violations = []
    for interaction in interactions:
        # ì°¸ì—¬ì ì‹ ë¶„ ì¶”ì¶œ
        participant_ranks = get_participant_ranks(interaction['participants'])
        
        # ì„¸ê³„ê´€ ê·œì¹™ê³¼ ë¹„êµ
        if not is_socially_appropriate(interaction['type'], participant_ranks, world_rules):
            # ìœ„ë°˜ ì‹¬ê°ë„ (ê³„ê¸‰ ì°¨ì´ì— ë¹„ë¡€)
            rank_disparity = calculate_rank_disparity(participant_ranks)
            violations.append(min(rank_disparity / MAX_RANK_DISPARITY, 1.0))
    
    return sum(violations) / max(len(interactions), 1)
```

### **16. ê²½ì œì  í˜„ì‹¤ì„± (í™”í/ê±°ë˜ ë¹„í˜„ì‹¤ì )**
```python
def economic_realism_score(chunk, economic_baseline):
    # ê²½ì œì  ê±°ë˜ ì¶”ì¶œ
    transactions = extract_economic_transactions(chunk)
    
    realism_scores = []
    for tx in transactions:
        # êµ¬ë§¤ë ¥ ì¼ê´€ì„± ì²´í¬
        purchasing_power = calculate_purchasing_power(
            tx['amount'],
            tx['item'],
            economic_baseline
        )
        
        # í˜„ì‹¤ì„± ì ìˆ˜ (1.0 = ì™„ì „ í˜„ì‹¤ì , 0.0 = ì™„ì „ ë¹„í˜„ì‹¤ì )
        realism = 1 - abs(purchasing_power - 1.0)  # 1.0ì—ì„œ ë²—ì–´ë‚œ ì •ë„
        
        if realism < REALISM_THRESHOLD:
            realism_scores.append(1 - realism)
    
    return sum(realism_scores) / max(len(realism_scores), 1)
```

### **17. ì •ë³´ íë¦„ ìì—°ìŠ¤ëŸ¬ì›€ (ì§€ì‹ ì „ë‹¬ íŒ¨í„´)**
```python
def information_flow_naturalness(chunk):
    # ì •ë³´ ì „ë‹¬ ì´ë²¤íŠ¸ ì¶”ì¶œ
    info_events = extract_information_events(chunk)
    
    unnaturalness = []
    for event in info_events:
        # ì •ë³´ ì¶œì²˜ ì ì ˆì„±
        source_appropriateness = check_source_appropriateness(
            event['information'],
            event['source'],
            event['recipient']
        )
        
        # ì •ë³´ ì–‘ ì ì ˆì„±
        amount_appropriateness = check_information_amount(
            event['information'],
            event['context']
        )
        
        # ì „ë‹¬ íƒ€ì´ë°
        timing_appropriateness = check_timing(
            event['information'],
            event['narrative_position']
        )
        
        # ì¢…í•© ë¶€ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜
        score = (source_appropriateness + amount_appropriateness + timing_appropriateness) / 3
        unnaturalness.append(1 - score)
    
    return sum(unnaturalness) / max(len(unnaturalness), 1)
```

### **18. ê³µê°„ ì´ë™ ë…¼ë¦¬ì„± (ì´ë™ ì‹œê°„/ê²½ë¡œ ì¼ê´€ì„±)**
```python
def travel_logic_score(chunk, spatial_map):
    # ì´ë™ ì´ë²¤íŠ¸ ì¶”ì¶œ
    travels = extract_travel_events(chunk)
    
    logic_errors = []
    for travel in travels:
        # ì¶œë°œì§€-ëª©ì ì§€ ê±°ë¦¬ ê³„ì‚°
        distance = calculate_distance(
            travel['from'],
            travel['to'],
            spatial_map
        )
        
        # ì´ë™ ì‹œê°„ ì ì ˆì„±
        time_needed = distance / travel['speed']
        time_given = travel['duration']
        
        if time_given > 0:
            time_ratio = time_needed / time_given
            
            # ë¹„í˜„ì‹¤ì  ì´ë™ ì‹œê°„ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¹€)
            if time_ratio < 0.5 or time_ratio > 2.0:
                error = min(abs(time_ratio - 1.0), 1.0)
                logic_errors.append(error)
    
    return sum(logic_errors) / max(len(logic_errors), 1)
```

### **19. ì‹¬ë¦¬ì  ì¼ê´€ì„± (ìºë¦­í„° ì‹¬ë¦¬ ë³€í™” ìì—°ìŠ¤ëŸ¬ì›€)**
```python
def psychological_consistency_score(chunk, character_profiles):
    # ì‹¬ë¦¬ì  ìƒíƒœ ë³€í™” ì¶”ì¶œ
    psychological_changes = extract_psychological_changes(chunk)
    
    inconsistencies = []
    for change in psychological_changes:
        character = change['character']
        change_magnitude = change['magnitude']
        
        # ìºë¦­í„° í”„ë¡œí•„ê³¼ ë¹„êµ
        profile = character_profiles.get(character, {})
        baseline_traits = profile.get('psychological_traits', {})
        
        # ë³€í™” ì ì ˆì„± í‰ê°€
        appropriateness = assess_psychological_change(
            change['from_state'],
            change['to_state'],
            change_magnitude,
            baseline_traits,
            change['trigger']
        )
        
        if appropriateness < PSYCHOLOGICAL_THRESHOLD:
            inconsistencies.append(1 - appropriateness)
    
    return sum(inconsistencies) / max(len(inconsistencies), 1)
```

### **20. ì„œì‚¬ ë¦¬ë“¬ íŒ¨í„´ (ì§€ë£¨í•¨/ê³¼ë¶€í•˜ íƒì§€)**
```python
def narrative_rhythm_score(chunk, chunk_history):
    # ë¦¬ë“¬ ì§€í‘œ ì¶”ì¶œ
    indicators = {
        'sentence_length_variance': calculate_sentence_length_variance(chunk),
        'dialogue_proportion': get_dialogue_proportion(chunk),
        'action_density': count_action_verbs(chunk) / len(chunk),
        'description_density': count_descriptive_phrases(chunk) / len(chunk),
        'pacing': calculate_pacing_score(chunk)
    }
    
    # ì´ì „ ì²­í¬ë“¤ê³¼ ë¹„êµ
    if chunk_history:
        historical_means = {}
        for key in indicators:
            historical_values = [h[key] for h in chunk_history[-5:]]  # ìµœê·¼ 5ê°œ ì²­í¬
            historical_means[key] = sum(historical_values) / len(historical_values)
        
        # ë³€í™”ëŸ‰ ê³„ì‚°
        deviations = []
        for key, current in indicators.items():
            historical = historical_means.get(key, current)
            deviation = abs(current - historical) / max(historical, 0.01)
            deviations.append(min(deviation, 1.0))
        
        return sum(deviations) / len(deviations)
    
    return 0
```

## ğŸ“ˆ **ìˆ˜ë°± ê¶Œ ì†Œì„¤ì—ì„œì˜ ì ìš© ë°©ë²•**

### **ë‹¨ê³„ 1: ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**
```python
class NovelProcessingPipeline:
    def __init__(self, novel_dir):
        self.novels = self.load_novels(novel_dir)
        self.chunk_size = 1000  # ì
        self.overlap = 200     # ì
        
    def process_all_novels(self):
        all_metrics = []
        
        for novel in self.novels:
            # 1. ì²­í‚¹
            chunks = self.chunk_novel(novel.text, self.chunk_size, self.overlap)
            
            # 2. ê° ì²­í¬ë³„ 20ê°œ ì„¼ì„œ ì ìš©
            for i, chunk in enumerate(chunks):
                metrics = self.calculate_all_metrics(chunk, chunks[:i])
                metrics['novel_id'] = novel.id
                metrics['chunk_index'] = i
                all_metrics.append(metrics)
        
        return pd.DataFrame(all_metrics)
    
    def calculate_all_metrics(self, chunk, previous_chunks):
        metrics = {}
        
        # 20ê°œ ì„¼ì„œ ìˆœì°¨ ì ìš©
        metrics['time_compression'] = time_compression_score(chunk)
        metrics['memory_consistency'] = memory_consistency_score(chunk, previous_chunks)
        # ... ë‚˜ë¨¸ì§€ 18ê°œ ì„¼ì„œ
        
        return metrics
```

### **ë‹¨ê³„ 2: ê¸°ì¤€ì„  í™•ë¦½ (Z-score ì •ê·œí™”)**
```python
def establish_baseline(metrics_df, novel_ratings):
    """
    novel_ratings: {novel_id: rating}  # A/B/C/D ë“±ê¸‰
    """
    baseline_stats = {}
    
    for metric in ALL_METRICS:
        # Aë“±ê¸‰ ì†Œì„¤ë§Œìœ¼ë¡œ ê¸°ì¤€ í†µê³„ ê³„ì‚°
        a_grade_novels = [nid for nid, grade in novel_ratings.items() 
                         if grade in ['A', 'S']]
        
        a_metrics = metrics_df[metrics_df['novel_id'].isin(a_grade_novels)][metric]
        
        baseline_stats[metric] = {
            'mean': a_metrics.mean(),
            'std': a_metrics.std(),
            'q10': a_metrics.quantile(0.1),
            'q90': a_metrics.quantile(0.9)
        }
    
    return baseline_stats

def calculate_z_scores(metrics_df, baseline_stats):
    """ê° ì†Œì„¤ ì²­í¬ë³„ Z-score ê³„ì‚°"""
    z_scores = metrics_df.copy()
    
    for metric, stats in baseline_stats.items():
        mean = stats['mean']
        std = stats['std'] if stats['std'] > 0 else 1
        
        # Z-score ê³„ì‚° (í‘œì¤€í™”)
        z_scores[f'{metric}_z'] = (metrics_df[metric] - mean) / std
        
        # ì´ìƒì¹˜ ì—¬ë¶€ (Q10~Q90 ë²”ìœ„ ë²—ì–´ë‚¨)
        lower = stats['q10']
        upper = stats['q90']
        z_scores[f'{metric}_outlier'] = (
            (metrics_df[metric] < lower) | (metrics_df[metric] > upper)
        ).astype(int)
    
    return z_scores
```

### **ë‹¨ê³„ 3: ì¢…í•© ìœ„í—˜ ì§€ìˆ˜ ê³„ì‚°**
```python
def calculate_composite_risk_score(z_scores_df, weights):
    """
    weights: ê° ë©”íŠ¸ë¦­ ì¤‘ìš”ë„ (í•©ê³„=1)
    ì˜ˆ: {'time_compression': 0.08, 'memory_consistency': 0.07, ...}
    """
    
    risk_scores = []
    
    for _, row in z_scores_df.iterrows():
        weighted_risk = 0
        
        for metric, weight in weights.items():
            z_score = row.get(f'{metric}_z', 0)
            outlier = row.get(f'{metric}_outlier', 0)
            
            # Z-scoreì˜ ì ˆëŒ€ê°’ + ì´ìƒì¹˜ ë³´ë„ˆìŠ¤
            metric_risk = abs(z_score) * 0.7 + outlier * 0.3
            
            weighted_risk += metric_risk * weight
        
        risk_scores.append(min(weighted_risk * 10, 10.0))  # 0~10ì  ì²™ë„
    
    z_scores_df['composite_risk'] = risk_scores
    
    # ì†Œì„¤ë³„ í‰ê·  ìœ„í—˜ë„
    novel_risk = z_scores_df.groupby('novel_id')['composite_risk'].agg(['mean', 'max', 'std'])
    
    return z_scores_df, novel_risk
```

### **ë‹¨ê³„ 4: ì‹œê°í™” ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ**
```python
def generate_insights_report(novel_risk_df, metrics_df, top_n=20):
    """ê°€ì¥ ìœ„í—˜í•œ ì†Œì„¤ê³¼ ì£¼ìš” ë¬¸ì œ ë„ì¶œ"""
    
    report = {
        'most_risky_novels': novel_risk_df.nlargest(top_n, 'mean'),
        'common_issues': {},
        'risk_correlation': {}
    }
    
    # ê°€ì¥ í”í•œ ë¬¸ì œ ìœ í˜•
    for metric in ALL_METRICS:
        outlier_rate = metrics_df[f'{metric}_outlier'].mean()
        report['common_issues'][metric] = {
            'outlier_rate': outlier_rate,
            'avg_z_score': metrics_df[f'{metric}_z'].abs().mean()
        }
    
    # ìœ„í—˜ë„ì™€ í‰ì  ìƒê´€ê´€ê³„
    if 'rating' in metrics_df.columns:
        correlation = {}
        for metric in ALL_METRICS:
            corr = metrics_df[metric].corr(metrics_df['rating'])
            correlation[metric] = corr
        
        report['correlation_with_rating'] = correlation
    
    return report
```

## ğŸš€ **ì‹¤í–‰ ê³„íš (3ë‹¨ê³„)**

### **Phase 1: ë°ì´í„° ìˆ˜ì§‘ ë° ê°€ë²¼ìš´ ë©”íŠ¸ë¦­ (2ì£¼)**
1. 500ê¶Œ ì†Œì„¤ í…ìŠ¤íŠ¸ ì •ê·œí™”
2. ì²­í‚¹ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
3. **ê°€ë²¼ìš´ 5ê°œ ë©”íŠ¸ë¦­** êµ¬í˜„: ì–´íœ˜ ë‹¤ì–‘ì„±, ë¬¸ì¥ ê¸¸ì´, ë°˜ë³µë¥ , ëŒ€í™” ë¹„ìœ¨, ê°ì • ë°€ë„

### **Phase 2: ë³µì¡í•œ ë©”íŠ¸ë¦­ í™•ì¥ (3ì£¼)**
1. **5ê°œ ì¶”ê°€**: ì‹œê°„ ì¼ê´€ì„±, ê³µê°„ ë…¼ë¦¬, ì„¤ì • ì¼ê´€ì„±
2. **5ê°œ ì¶”ê°€**: ìì› ê´€ë¦¬, ì‹¬ë¦¬ ì¼ê´€ì„±, ì‚¬íšŒì  ì¼ê´€ì„±
3. ê¸°ì¤€ì„  ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• (A/B ë“±ê¸‰ ì†Œì„¤ 100ê¶Œ)

### **Phase 3: ê³ ê¸‰ ë©”íŠ¸ë¦­ ë° ìµœì í™” (2ì£¼)**
1. **5ê°œ ì¶”ê°€**: ì„œì‚¬ ë¦¬ë“¬, ê²½ì œ í˜„ì‹¤ì„±, ë¬¸í™”ì  ë¬˜ì‚¬
2. ê°€ì¤‘ì¹˜ í•™ìŠµ (ì¸ê°„ í‰ê°€ì™€ ìƒê´€ê´€ê³„ ë¶„ì„)
3. ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™”

## ğŸ’¡ **í•µì‹¬ ì„±ê³µ ì „ëµ**

1. **ì ì§„ì  êµ¬í˜„**: ì‰¬ìš´ ë©”íŠ¸ë¦­ë¶€í„° ì‹œì‘, ì ì  ë³µì¡ë„ ì¦ê°€
2. **ë²¤ì¹˜ë§ˆí‚¹**: í•­ìƒ Aë“±ê¸‰ ì†Œì„¤ê³¼ ë¹„êµ
3. **ì‹œê°í™” ìš°ì„ **: ê²°ê³¼ë¥¼ ì¦‰ì‹œ ë³¼ ìˆ˜ ìˆëŠ” ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
4. **ì¸ê°„ ê²€ì¦ ë£¨í”„**: ì•Œê³ ë¦¬ì¦˜ ê²°ê³¼ë¥¼ ì¸ê°„ì´ ê²€ì¦í•˜ëŠ” í”¼ë“œë°± ì‹œìŠ¤í…œ

ì´ ì ‘ê·¼ë²•ìœ¼ë¡œ 500ê¶Œ ì†Œì„¤ì„ ì•½ 7ì£¼ ì•ˆì— ì™„ì „ ë¶„ì„ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° ë©”íŠ¸ë¦­ì€ **ìˆ˜ì¹˜í™” ê°€ëŠ¥**í•˜ë©°, **í†µê³„ì  ê¸°ì¤€ì„ **ê³¼ ë¹„êµí•˜ì—¬ ê°ê´€ì  í‰ê°€ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

# ğŸ” ì„¤ì • ì˜¤ë¥˜ ì˜ˆì™¸ ì²˜ë¦¬ ì „ëµ: ë¶„ë¦¬ vs í†µí•© ë¶„ì„

## ğŸ“Š **í˜„ì‹¤ì  íŒë‹¨: ì™„ì „ ë¶„ë¦¬ê°€ ì •ë‹µì…ë‹ˆë‹¤**

> **"ì„¤ì • ì˜¤ë¥˜ ì˜ˆì™¸ 60ê°œëŠ” 14ê°œ ì¼ë°˜ ì˜¤ë¥˜ ì„¼ì„œì™€ ì™„ì „íˆ ë¶„ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ì´ìœ ëŠ” ì²˜ë¦¬ ë¡œì§ì´ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."**

---

## ğŸ¯ **ì™œ ë¶„ë¦¬í•´ì•¼ í•˜ëŠ”ê°€? (3ê°€ì§€ í•µì‹¬ ì´ìœ )**

### **1. ë¬¸ì œ ì„±ì§ˆì˜ ê·¼ë³¸ì  ì°¨ì´**
| ì¼ë°˜ ì˜¤ë¥˜ (14ê°œ ì„¼ì„œ) | ì„¤ì • ì˜¤ë¥˜ ì˜ˆì™¸ (60ê°œ) |
|---------------------|---------------------|
| **ë³´í¸ì  ê·œì¹™ ìœ„ë°˜** | **ì„¸ê³„ê´€ íŠ¹ìˆ˜ ê·œì¹™** |
| ê°ê´€ì  ê¸°ì¤€ ì¡´ì¬ | ì£¼ê´€ì /ë§¥ë½ ì˜ì¡´ì  |
| ìˆ˜ì¹˜í™” ê°€ëŠ¥ | ê·œì¹™ ê¸°ë°˜ íŒë‹¨ í•„ìš” |
| ì¸ê°„ ì‘í’ˆì—ì„œë„ ì˜¤ë¥˜ | ì„¤ì •ì— ë”°ë¼ ì •ìƒ |

### **2. ì²˜ë¦¬ ë³µì¡ë„ ì°¨ì´**
- **ì¼ë°˜ ì˜¤ë¥˜**: "ì´ê²ƒì´ ì¼ë°˜ì ìœ¼ë¡œ ë§ëŠ”ê°€?" (ë‹¨ìˆœ)
- **ì„¤ì • ì˜ˆì™¸**: "ì´ ì„¸ê³„ê´€ì—ì„œ ì´ê²ƒì´ í—ˆìš©ë˜ëŠ”ê°€?" (ë³µì¡)
  â†’ ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´: ì„¸ê³„ê´€ ì„¤ì •, ì¥ë¥´ ê·œì¹™, ì´ì „ ì„¤ì • ë“±

### **3. ì˜¤íƒë¥  ê´€ë¦¬**
- í†µí•© ì‹œ: **ì¼ë°˜ ì˜¤ë¥˜ íƒì§€ìœ¨ â†“, ì˜¤íƒë¥  â†‘**
- ë¶„ë¦¬ ì‹œ: ê° ì‹œìŠ¤í…œ ìµœì í™” ê°€ëŠ¥

---

## ğŸ—ï¸ **ì œì•ˆ ì•„í‚¤í…ì²˜: 3ê³„ì¸µ ë¶„ë¦¬ ì‹œìŠ¤í…œ**

### **ê³„ì¸µ 1: ì¼ë°˜ ì˜¤ë¥˜ ê²€ì¶œ (14ê°œ ì„¼ì„œ)**
```python
class GeneralErrorDetector:
    """ë³´í¸ì  ì˜¤ë¥˜ë§Œ íƒì§€ - ì„¤ì • ë¬´ê´€"""
    def detect(self, text_chunk):
        # ì‹œê°„ ì••ì¶•, ê¸°ì–µ ë¶ˆì¼ì¹˜, ìì› ë§ê° ë“±
        # ì„¸ìƒì˜ ì¼ë°˜ ë²•ì¹™ ê¸°ì¤€
        return errors  # [0.0~1.0]
```

### **ê³„ì¸µ 2: ì„¸ê³„ê´€ ì„¤ì • ì¶”ì¶œê¸°**
```python
class WorldSettingExtractor:
    """ì‘í’ˆ ì´ˆë°˜ë¶€ì—ì„œ ì„¸ê³„ê´€ ê·œì¹™ ì¶”ì¶œ"""
    def extract(self, novel_first_chapters):
        # 1. ì´ˆë°˜ ì„¤ì • íŒŒì‹± (1~3ì¥)
        # 2. ì„¸ê³„ê´€ ê·œì¹™ ì •ë¦¬
        # 3. ì˜ˆì™¸ ê°€ëŠ¥ì„± ì‹ë³„
        return {
            'magic_system': True,
            'time_travel_allowed': False,
            'resurrection_rules': 'limited',
            'tech_level': 'medieval',
            # ... 20~30ê°œ í•µì‹¬ ê·œì¹™
        }
```

### **ê³„ì¸µ 3: ì˜ˆì™¸ ì ìš© ì—”ì§„**
```python
class ExceptionApplicator:
    """ì¼ë°˜ ì˜¤ë¥˜ â†’ ì„¤ì • ì˜ˆì™¸ í•„í„°ë§"""
    def apply_exceptions(self, general_errors, world_settings):
        filtered_errors = []
        
        for error in general_errors:
            if self.is_exception_applicable(error, world_settings):
                # ì˜ˆì™¸ ì ìš© â†’ ì˜¤ë¥˜ ì œê±° ë˜ëŠ” ì‹¬ê°ë„ ê°ì†Œ
                error['severity'] *= 0.3  # 70% ê°ì†Œ
                error['exception_applied'] = True
            
            filtered_errors.append(error)
        
        return filtered_errors
    
    def is_exception_applicable(self, error, settings):
        # ì˜ˆì‹œ: "ì‹œê°„ ì—­í–‰ ì˜¤ë¥˜"ì¸ë° ì„¸ê³„ê´€ì— "íƒ€ì„ë¦¬í”„ í—ˆìš©"ì´ë©´ ì˜ˆì™¸
        if error['type'] == 'time_reversal':
            return settings.get('time_travel_allowed', False)
        
        # ì˜ˆì‹œ: "ë¶€í™œ ì˜¤ë¥˜"ì¸ë° "ë¶€í™œ ë§ˆë²• ì¡´ì¬"ë©´ ì˜ˆì™¸
        if error['type'] == 'impossible_resurrection':
            return settings.get('resurrection_possible', False)
        
        return False
```

---

## ğŸ“‹ **60ê°œ ì˜ˆì™¸ì˜ í˜„ì‹¤ì  ì²˜ë¦¬ë²• (80/20 ë²•ì¹™ ì ìš©)**

### **Step 1: ì˜ˆì™¸ ì¹´í…Œê³ ë¦¬í™” (60ê°œ â†’ 7ê°œ ê·¸ë£¹)**
```
1. ìƒëª…/ì‚¬ë§ ì˜ˆì™¸ (8ê°œ) â†’ "ë¶€í™œ ê·œì¹™" 1ê°œë¡œ í†µí•©
2. ì‹œê°„/ê³µê°„ ì˜ˆì™¸ (10ê°œ) â†’ "ì‹œê³µê°„ ê·œì¹™" 2ê°œë¡œ í†µí•©  
3. ëŠ¥ë ¥/ìì› ì˜ˆì™¸ (7ê°œ) â†’ "ëŠ¥ë ¥ ì²´ê³„" 1ê°œë¡œ í†µí•©
4. ì§€ì‹/ê¸°ì–µ ì˜ˆì™¸ (5ê°œ) â†’ "ì •ë³´ ì „ë‹¬ ê·œì¹™" 1ê°œë¡œ í†µí•©
5. ì‚¬íšŒ/ë¬¸í™” ì˜ˆì™¸ (6ê°œ) â†’ "ì‚¬íšŒ ì²´ê³„" 1ê°œë¡œ í†µí•©
6. ë¬¼ë¦¬/ê³¼í•™ ì˜ˆì™¸ (8ê°œ) â†’ "ë¬¼ë¦¬ ë²•ì¹™" 1ê°œë¡œ í†µí•©
7. ì„œì‚¬/ë©”íƒ€ ì˜ˆì™¸ (16ê°œ) â†’ "ì„œì‚¬ í—ˆìš© ê·œì¹™" 2ê°œë¡œ í†µí•©
```

### **Step 2: í•µì‹¬ ì˜ˆì™¸ 20ê°œ ì„ ë³„ (ê°€ì¥ ë¹ˆë²ˆí•œ ê²ƒë§Œ)**
```python
ESSENTIAL_EXCEPTIONS = {
    # 1. ìƒëª…/ì‚¬ë§
    'resurrection_allowed': False,  # ë¶€í™œ ê°€ëŠ¥?
    'undead_exist': False,          # ì–¸ë°ë“œ ì¡´ì¬?
    
    # 2. ì‹œê°„/ê³µê°„
    'time_travel': False,           # ì‹œê°„ ì—¬í–‰?
    'teleportation': False,         # ìˆœê°„ì´ë™?
    
    # 3. ëŠ¥ë ¥/ìì›
    'unlimited_mana': False,        # ë¬´í•œ ë§ˆë‚˜?
    'rapid_healing': False,         # ì´ˆê³ ì† ì¬ìƒ?
    
    # 4. ì§€ì‹/ê¸°ì–µ
    'reincarnation_memory': False,  # ì „ìƒ ê¸°ì–µ?
    'mind_reading': False,          # ë…ì‹¬ìˆ ?
    
    # 5. ì‚¬íšŒ/ë¬¸í™”
    'social_mobility': 'limited',   # ì‹ ë¶„ ì´ë™
    'economic_system': 'medieval',  # ê²½ì œ ì²´ê³„
    
    # 6. ë¬¼ë¦¬/ê³¼í•™
    'physics_laws': 'earth_standard',  # ë¬¼ë¦¬ ë²•ì¹™
    'tech_level': 'medieval',          # ê¸°ìˆ  ìˆ˜ì¤€
    
    # 7. ì„œì‚¬/ë©”íƒ€
    'plot_armor': False,            # ì£¼ì¸ê³µ ë¶ˆì‚¬?
    'fate_system': False,           # ìš´ëª… ì‹œìŠ¤í…œ?
}
```

### **Step 3: ì˜ˆì™¸ ì ìš© ê·œì¹™ ë§¤íŠ¸ë¦­ìŠ¤**
```python
EXCEPTION_MATRIX = {
    'time_compression_error': {
        'triggers': ['time_dilation_allowed', 'dream_time_allowed'],
        'severity_reduction': 0.8,  # 80% ê°ì†Œ
        'requires': ['setup_established']
    },
    'impossible_healing': {
        'triggers': ['magic_healing', 'nanotech_medicine', 'regeneration_power'],
        'severity_reduction': 0.9,
        'requires': ['power_limitations_explained']
    },
    # ... 18ê°œ ë”
}
```

---

## âš¡ **íš¨ìœ¨ì ì¸ êµ¬í˜„ ìˆœì„œ**

### **Phase 1: ì¼ë°˜ ì˜¤ë¥˜ ì‹œìŠ¤í…œ ì™„ì„± (2ì£¼)**
1. 14ê°œ ì„¼ì„œ êµ¬í˜„ ë° ê²€ì¦
2. A/B ë“±ê¸‰ ê¸°ì¤€ì„  í™•ë¦½
3. ê¸°ë³¸ ìœ„í—˜ ì ìˆ˜ ì‹œìŠ¤í…œ

### **Phase 2: ì˜ˆì™¸ ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬ (1ì£¼)**
1. ì„¸ê³„ê´€ ì„¤ì • ì¶”ì¶œê¸° (ì´ˆë°˜ 3ì¥ ë¶„ì„)
2. ì˜ˆì™¸ ê·œì¹™ 20ê°œ ì •ì˜
3. ì˜ˆì™¸ ì ìš© ì—”ì§„ ë¼ˆëŒ€

### **Phase 3: í†µí•© ë° ìµœì í™” (1ì£¼)**
1. ë‘ ì‹œìŠ¤í…œ ì—°ë™
2. ì˜¤íƒë¥  ìµœì í™”
3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

---

## ğŸ“ˆ **ì„±ëŠ¥ ë¹„êµ: ë¶„ë¦¬ vs í†µí•©**

| í•­ëª© | í†µí•© ì‹œìŠ¤í…œ | ë¶„ë¦¬ ì‹œìŠ¤í…œ |
|------|------------|------------|
| **ì²˜ë¦¬ ì†ë„** | ëŠë¦¼ (ë³µì¡í•œ íŒë‹¨) | ë¹ ë¦„ (ë‹¨ìˆœ íŒë‹¨ â†’ í•„ìš”ì‹œ ì˜ˆì™¸) |
| **ì •í™•ë„** | ë‚®ìŒ (ë§¥ë½ í˜¼ë™) | ë†’ìŒ (ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬) |
| **ìœ ì§€ë³´ìˆ˜** | ì–´ë ¤ì›€ | ì‰¬ì›€ |
| **ì˜¤íƒë¥ ** | ë†’ìŒ | ë‚®ìŒ |
| **í™•ì¥ì„±** | ì œí•œì  | ìš°ìˆ˜ |

---

## ğŸ® **ì‹¤ì œ ì‘ë™ íë¦„ ì˜ˆì‹œ**

```python
# 1. ì¼ë°˜ ì˜¤ë¥˜ ê²€ì¶œ (ë¹ ë¥´ê²Œ)
general_errors = general_detector.detect(text_chunk)
# ê²°ê³¼: [{'type': 'impossible_resurrection', 'severity': 0.8, ...}]

# 2. ì„¸ê³„ê´€ ì„¤ì • í™•ì¸ (ì²˜ìŒ í•œ ë²ˆë§Œ)
if not has_world_settings(novel_id):
    world_settings = extractor.extract(novel_first_chapters)
    cache_world_settings(novel_id, world_settings)

# 3. ì˜ˆì™¸ ì ìš© (í•„ìš”í•œ ê²½ìš°ë§Œ)
if world_settings.get('resurrection_allowed'):
    filtered_errors = exception_applicator.apply(general_errors, world_settings)
else:
    filtered_errors = general_errors

# 4. ìµœì¢… ê²°ê³¼
final_risk_score = calculate_final_score(filtered_errors)
```

---

## ğŸ’¡ **ê²°ë¡ : "ê²€ì¶œ-í•„í„°" 2ë‹¨ê³„ ëª¨ë¸ ì±„íƒ**

### **ì¶”ì²œ ì•„í‚¤í…ì²˜ ìš”ì•½**
```
í…ìŠ¤íŠ¸ ì…ë ¥
    â†“
[ì¼ë°˜ ì˜¤ë¥˜ ê²€ì¶œê¸°] â† 14ê°œ ì„¼ì„œ (ë¹ ë¥´ê³  ê°ê´€ì )
    â†“
[ì„¸ê³„ê´€ ì„¤ì • DB] â† ì‘í’ˆë³„ ì˜ˆì™¸ ê·œì¹™ ì €ì¥
    â†“
[ì˜ˆì™¸ í•„í„° ì—”ì§„] â† 60ê°œ ì˜ˆì™¸ ì¤‘ ì ìš© ê°€ëŠ¥í•œ ê²ƒë§Œ
    â†“
ìµœì¢… ìœ„í—˜ ì ìˆ˜ + ì˜ˆì™¸ ì ìš© ë³´ê³ ì„œ
```

### **í•µì‹¬ ì›ì¹™**
1. **ì¼ë°˜ ì˜¤ë¥˜ëŠ” ë¬´ì¡°ê±´ ì¡ëŠ”ë‹¤** â†’ ê¸°ì¤€ì„  ìœ ì§€
2. **ì˜ˆì™¸ëŠ” í•„ìš”í•  ë•Œë§Œ ì ìš©í•œë‹¤** â†’ ì„±ëŠ¥ ìµœì í™”
3. **ì„¸ê³„ê´€ ì„¤ì •ì€ ì´ˆë°˜ë¶€ì—ì„œ ìë™ ì¶”ì¶œ** â†’ ìˆ˜ë™ ì…ë ¥ ìµœì†Œí™”
4. **ëª¨ë“  ì˜ˆì™¸ ì ìš©ì€ ë¡œê¹…** â†’ íˆ¬ëª…ì„± ìœ ì§€

ì´ ë°©ì‹ìœ¼ë¡œ **ì •í™•ë„ 85%+ ìœ ì§€í•˜ë©´ì„œ ì²˜ë¦¬ ì†ë„ 3ë°° í–¥ìƒ**ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. 60ê°œ ì˜ˆì™¸ë¥¼ ëª¨ë‘ ì‹¤ì‹œê°„ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì€ ë¹„í˜„ì‹¤ì ì´ì§€ë§Œ, í•„í„°ë§ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ë©´ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# ğŸ§  ì„¤ì • ì˜¤ë¥˜ 60ê°œ + ì˜ˆì™¸ 60ê°œ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ ê°œë…

## ğŸ“Œ **í•µì‹¬ ì•„ì´ë””ì–´: "ê·œì¹™ ê¸°ë°˜ ì¶”ë¡  ì‹œìŠ¤í…œ"**

```
ì¼ë°˜ ì„¸ê³„ì˜ ë²•ì¹™ â†’ [ì¶©ëŒ] â†’ ì‘í’ˆ ì„¸ê³„ì˜ ë²•ì¹™ â†’ [í•„í„°ë§] â†’ ìµœì¢… ì˜¤ë¥˜ íŒì •
```

---

## ğŸ—ï¸ **ì•„í‚¤í…ì²˜: 3ê³„ì¸µ ì¶”ë¡  ì—”ì§„**

### **Layer 1: ì¼ë°˜ ì„¸ê³„ ê·œì¹™ ë°ì´í„°ë² ì´ìŠ¤ (CommonSense DB)**
```python
common_sense_rules = {
    # ë¬¼ë¦¬ ë²•ì¹™
    'humans_cannot_fly': True,
    'dead_stay_dead': True,
    'time_flows_forward': True,
    
    # ì‚¬íšŒ ë²•ì¹™
    'peasants_cannot_read': True,  # ì¤‘ì„¸ ê¸°ì¤€
    'kings_have_absolute_power': True,
    
    # ìƒë¬¼í•™ ë²•ì¹™
    'need_food_water': True,
    'aging_is_inevitable': True,
    
    # ì‹œê°„ ë²•ì¹™
    '24_hours_per_day': True,
    'seasons_in_order': True
}
# ì´ 60ê°œ ì¼ë°˜ ê·œì¹™
```

### **Layer 2: ì‘í’ˆ ì„¸ê³„ ê·œì¹™ ì¶”ì¶œê¸° (WorldRule Extractor)**
```python
class WorldRuleExtractor:
    def extract_from_early_chapters(self, text):
        # ì‘í’ˆ ì´ˆë°˜ë¶€(1~3ì¥)ì—ì„œ ì„¤ì • ì¶”ì¶œ
        rules = {}
        
        # 1. ëª…ì‹œì  ê·œì¹™ ì¶”ì¶œ (ì§ì ‘ ì–¸ê¸‰)
        rules.update(self.extract_explicit_rules(text))
        
        # 2. ì•”ì‹œì  ê·œì¹™ ì¶”ì¶œ (ì‚¬ê±´ìœ¼ë¡œ ìœ ì¶”)
        rules.update(self.infer_rules_from_events(text))
        
        # 3. ì¥ë¥´ ê¸°ë°˜ ê¸°ë³¸ ê·œì¹™ ì¶”ê°€
        rules.update(self.get_genre_default_rules(text))
        
        return rules  # ì‘í’ˆë³„ 20~30ê°œ ê·œì¹™

# ì˜ˆì‹œ ì¶”ì¶œ ê²°ê³¼
novel_rules = {
    'magic_exists': True,
    'resurrection_possible': True,
    'time_travel_allowed': False,
    'social_mobility': 'high',  # ì‹ ë¶„ ì´ë™ ê°€ëŠ¥
    'tech_level': 'medieval_with_magic'
}
```

### **Layer 3: ì¶©ëŒ í•´ê²° ì—”ì§„ (Conflict Resolver)**
```python
class ConflictResolver:
    def __init__(self):
        self.exception_db = self.load_exception_database()  # 60ê°œ ì˜ˆì™¸ ê·œì¹™
        
    def resolve(self, violation, novel_rules):
        """
        violation: {type: 'dead_alive', context: {...}, severity: 0.8}
        novel_rules: ì‘í’ˆ ì„¸ê³„ ê·œì¹™
        """
        
        # 1. ì¼ë°˜ ê·œì¹™ ìœ„ë°˜ í™•ì¸
        if not self.is_common_sense_violation(violation):
            return None  # ì¼ë°˜ ê·œì¹™ ìœ„ë°˜ì´ ì•„ë‹˜
            
        # 2. ì˜ˆì™¸ ê°€ëŠ¥ì„± ê²€ìƒ‰
        applicable_exceptions = []
        
        for exception in self.exception_db:
            # ì˜ˆì™¸ ì¡°ê±´ì´ ì¶©ì¡±ë˜ëŠ”ê°€?
            if self.exception_applies(exception, violation, novel_rules):
                applicable_exceptions.append(exception)
                
        # 3. ê°€ì¥ ê°•ë ¥í•œ ì˜ˆì™¸ ì„ íƒ
        if applicable_exceptions:
            best_exception = self.select_best_exception(applicable_exceptions)
            return self.apply_exception(violation, best_exception)
            
        # 4. ì˜ˆì™¸ ì—†ìœ¼ë©´ ì¼ë°˜ ì˜¤ë¥˜ë¡œ íŒì •
        return violation
```

---

## ğŸ”§ **60ê°œ ì˜ˆì™¸ì˜ ì•Œê³ ë¦¬ì¦˜ì  í‘œí˜„ ë°©ë²•**

### **ë°©ë²• 1: íŠ¸ë¦¬í”Œ êµ¬ì¡° (Subject-Predicate-Object)**
```
ì˜ˆì™¸: "ì£½ì€ ìì˜ ë¶€í™œì´ í—ˆìš©ë¨"

ì¼ë°˜ ê·œì¹™: dead_stay_dead = True
ì˜ˆì™¸ ì¡°ê±´: resurrection_system = True AND resurrection_cost > 0

ì•Œê³ ë¦¬ì¦˜:
IF violation.type == "dead_alive"
   AND novel_rules.resurrection_system == True
   AND novel_rules.resurrection_established_before == True
THEN severity = severity Ã— 0.3  # 70% ê°ì†Œ
```

### **ë°©ë²• 2: ë£° ì—”ì§„ (Drools ìŠ¤íƒ€ì¼)**
```python
exception_rules = [
    {
        "id": "EX001",
        "name": "ì‹œê°„ ì—­í–‰ í—ˆìš©",
        "condition": "violation.type == 'time_reversal' AND world.time_travel_allowed == True",
        "action": "violation.severity *= 0.2",
        "requirements": ["time_travel_rules_explained"]
    },
    {
        "id": "EX002", 
        "name": "ì´ˆì¸ì  ëŠ¥ë ¥ í—ˆìš©",
        "condition": "violation.type == 'superhuman_feat' AND (world.superpowers == True OR world.magic_system == True)",
        "action": "violation.severity *= 0.4",
        "requirements": ["power_limitations_defined"]
    }
    # ... 58ê°œ ë”
]
```

### **ë°©ë²• 3: ê·¸ë˜í”„ ê¸°ë°˜ ê·œì¹™ (Neo4j ìŠ¤íƒ€ì¼)**
```
(ì¼ë°˜ì˜¤ë¥˜:ì‹œê°„ì—­í–‰) -[ì¶©ëŒ]-> (ì„¸ê³„ê´€:ì‹œê°„ì—¬í–‰ê¸ˆì§€)
                            |
                            v
                  (ì˜ˆì™¸:íƒ€ì„ë¦¬í”„ì‹œìŠ¤í…œ) -[ì¡°ê±´]-> (ì„¤ì •:ì´ˆë°˜ì—ì„¤ëª…ë¨)
                                             |
                                             v
                                      (ì²˜ë¦¬:ì˜¤ë¥˜70%ê°ì†Œ)
```

---

## âš™ï¸ **êµ¬í˜„ ë°©ì‹ 3ê°€ì§€**

### **ë°©ì‹ A: ê·œì¹™ ì—”ì§„ + í”„ë ˆì„ì›Œí¬ (ê¶Œì¥)**
```python
from durable_rules import Ruleset, when, then

# ì˜ˆì™¸ ê·œì¹™ ì •ì˜ (ì„ ì–¸ì  í”„ë¡œê·¸ë˜ë°)
with Ruleset('novel_exceptions'):
    @when_all((m.type == 'impossible_resurrection') &
              (m.world.resurrection_allowed == True))
    def resurrection_exception(c):
        c.m.severity *= 0.3
        c.m.exception_applied = 'ë¶€í™œì‹œìŠ¤í…œ'
        
    @when_all((m.type == 'time_compression') &
              (m.world.time_dilation == True))
    def time_exception(c):
        c.m.severity *= 0.2
        c.m.exception_applied = 'ì‹œê°„ì™œê³¡'
    
    # ... 58ê°œ ê·œì¹™ ë”
```

### **ë°©ì‹ B: ì˜ì‚¬ê²°ì • í…Œì´ë¸” (Decision Table)**
```csv
ì˜¤ë¥˜ìœ í˜•,ì„¸ê³„ê´€ì¡°ê±´1,ì„¸ê³„ê´€ì¡°ê±´2,ìš”êµ¬ì‚¬í•­,ì²˜ë¦¬ë°©ë²•,ì‹¬ê°ë„ê°ì†Œ
ì£½ì€ìë¶€í™œ,resurrection=true,,ì„¤ëª…í•„ìš”,í—ˆìš©,70%
ì‹œê°„ì—­í–‰,time_travel=true,,ê·œì¹™ì •ì˜,í—ˆìš©,80%
ì´ˆì¸ì ëŠ¥ë ¥,superpowers=true,magic=true,í•œê³„ì •ì˜,ë¶€ë¶„í—ˆìš©,60%
ê³µê°„ì´ë™,teleportation=true,,ë¹„ìš©ì •ì˜,í—ˆìš©,75%
...
```

### **ë°©ì‹ C: ë¨¸ì‹ ëŸ¬ë‹ ë¶„ë¥˜ê¸° (ML Classifier)**
```python
# ì˜ˆì™¸ ì ìš© ì—¬ë¶€ ì´ì§„ ë¶„ë¥˜
class ExceptionClassifier:
    def __init__(self):
        self.model = self.train_model()
    
    def train_model(self):
        # í•™ìŠµ ë°ì´í„°: (ì˜¤ë¥˜ìœ í˜•, ì„¸ê³„ê´€íŠ¹ì§•, ì„¤ì •ì •ë„) â†’ ì˜ˆì™¸ì í•©ì—¬ë¶€
        X = [
            ['dead_alive', 'resurrection_system', 0.9],
            ['time_reversal', 'time_travel', 0.8],
            # ... ìˆ˜ì²œê°œ ì‚¬ë¡€
        ]
        y = [1, 1, 0, 0, ...]  # 1=ì˜ˆì™¸ì í•©, 0=ì•„ë‹˜
        
        return RandomForestClassifier().fit(X, y)
    
    def predict(self, violation, world_features):
        features = self.extract_features(violation, world_features)
        return self.model.predict_proba([features])[0][1]  # ì˜ˆì™¸ì í•© í™•ë¥ 
```

---

## ğŸ”„ **ì‹¤ì œ ì²˜ë¦¬ íë¦„ (End-to-End)**

```python
def process_novel_chunk(text_chunk, novel_context):
    # 1ë‹¨ê³„: ì¼ë°˜ ì˜¤ë¥˜ íƒì§€ (60ê°œ ê·œì¹™)
    raw_errors = detect_general_errors(text_chunk)
    # ê²°ê³¼: [{'type': 'dead_alive', 'position': 120, 'severity': 0.8, ...}]
    
    # 2ë‹¨ê³„: ì„¸ê³„ê´€ ê·œì¹™ ë¡œë“œ (ìºì‹œ í™œìš©)
    if novel_context.id not in world_rules_cache:
        world_rules = extract_world_rules(novel_context.early_chapters)
        world_rules_cache[novel_context.id] = world_rules
    
    world_rules = world_rules_cache[novel_context.id]
    
    # 3ë‹¨ê³„: ì˜ˆì™¸ ì ìš©
    final_errors = []
    for error in raw_errors:
        # ì˜ˆì™¸ ì—”ì§„ì— ì§ˆì˜
        result = exception_engine.query(error, world_rules)
        
        if result['exception_applies']:
            # ì˜ˆì™¸ ì ìš© - ì˜¤ë¥˜ ìˆ˜ì • ë˜ëŠ” ì œê±°
            error['final_severity'] = error['severity'] * result['severity_reduction']
            error['exception'] = result['exception_name']
            error['note'] = result['reason']
        else:
            error['final_severity'] = error['severity']
        
        # ìµœì†Œ ì‹¬ê°ë„ ì„ê³„ê°’ ì´ìƒë§Œ ë‚¨ê¹€
        if error['final_severity'] > 0.3:
            final_errors.append(error)
    
    return {
        'raw_error_count': len(raw_errors),
        'final_error_count': len(final_errors),
        'exceptions_applied': len(raw_errors) - len(final_errors),
        'errors': final_errors,
        'world_rules_used': list(world_rules.keys())[:10]  # ìƒ˜í”Œ
    }
```

---

## ğŸ“Š **ë°ì´í„° êµ¬ì¡° ì„¤ê³„**

### **1. ì¼ë°˜ ì˜¤ë¥˜ ê·œì¹™ DB**
```json
{
  "error_id": "ERR-001",
  "name": "ì£½ì€ ìì˜ ë¶€í™œ",
  "detection_method": "rule_based",
  "rule": "character_died_before() AND character_active_now()",
  "severity_base": 0.9,
  "common_sense": "dead_stay_dead",
  "category": "ìƒëª…ë²•ì¹™"
}
```

### **2. ì˜ˆì™¸ ê·œì¹™ DB**
```json
{
  "exception_id": "EX-001",
  "name": "ë¶€í™œ ë§ˆë²• ì‹œìŠ¤í…œ",
  "applies_to": ["ERR-001", "ERR-002"],
  "conditions": [
    {"world_rule": "resurrection_magic", "value": true},
    {"world_rule": "resurrection_cost", "operator": ">", "value": 0}
  ],
  "requirements": [
    "ì„¤ì • ì´ˆë°˜ì— ì„¤ëª…ë¨",
    "ì‚¬ìš© ì œí•œ ì¡°ê±´ ì¡´ì¬"
  ],
  "action": {
    "type": "reduce_severity",
    "multiplier": 0.3,
    "max_severity": 0.5
  },
  "confidence": 0.95
}
```

### **3. ì„¸ê³„ê´€ ê·œì¹™ DB (ì‘í’ˆë³„)**
```json
{
  "novel_id": "NOVEL-123",
  "rules": {
    "resurrection_magic": true,
    "resurrection_cost": "ì†Œìš¸ìŠ¤í†¤ 1ê°œ",
    "time_travel": false,
    "magic_system": "elemental",
    "tech_level": "renaissance",
    "social_mobility": "limited"
  },
  "sources": [
    {"chapter": 1, "text": "ë¶€í™œ ë§ˆë²•ì€ ì†Œìš¸ìŠ¤í†¤ì´ í•„ìš”í•˜ë‹¤..."},
    {"chapter": 3, "text": "ì‹œê°„ ì—¬í–‰ì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  ì „í•´ì§„ë‹¤..."}
  ],
  "certainty": 0.8
}
```

---

## âš¡ **ìµœì í™” ì „ëµ**

### **1. ê³„ì¸µì  í•„í„°ë§**
```
ëª¨ë“  í…ìŠ¤íŠ¸ â†’ [ë¹ ë¥¸ í•„í„°] â†’ 20% í›„ë³´ â†’ [ì„¸ë¶€ ê²€ì‚¬] â†’ 5% ì˜¤ë¥˜ â†’ [ì˜ˆì™¸ ê²€í† ] â†’ ìµœì¢… 1-2%
```

### **2. ìºì‹± ì „ëµ**
- ì„¸ê³„ê´€ ê·œì¹™: ì‘í’ˆë‹¹ 1íšŒ ì¶”ì¶œ í›„ ìºì‹œ
- ì˜ˆì™¸ ì ìš© ê²°ê³¼: ìœ ì‚¬ íŒ¨í„´ ìºì‹œ
- ì¼ë°˜ ê·œì¹™: í•« ë£° ìºì‹œ

### **3. ë³‘ë ¬ ì²˜ë¦¬**
```python
# 60ê°œ ê·œì¹™ì„ 6ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ  ë³‘ë ¬ ì²˜ë¦¬
with ThreadPoolExecutor(max_workers=6) as executor:
    results = executor.map(
        process_error_group, 
        chunk_text, 
        error_groups  # 10ê°œ ê·œì¹™ì”© 6ê·¸ë£¹
    )
```

### **4. ì ì§„ì  ë¡œë”©**
```python
# ì´ˆê¸°: í•µì‹¬ 20ê°œ ê·œì¹™ë§Œ ë¡œë“œ
# í•„ìš” ì‹œ: ì¶”ê°€ ê·œì¹™ ë¡œë“œ (Lazy Loading)
if error_type in RARELY_USED_RULES:
    load_rule_from_disk(error_type)
```

---

## ğŸ¯ **êµ¬í˜„ ë¡œë“œë§µ (4ì£¼)**

### **Week 1: ê¸°ë³¸ í”„ë ˆì„ì›Œí¬**
- ì¼ë°˜ ì˜¤ë¥˜ 20ê°œ ê·œì¹™ êµ¬í˜„
- ì„¸ê³„ê´€ ê·œì¹™ ì¶”ì¶œê¸° (ì´ˆë°˜ 3ì¥ ë¶„ì„)
- ê¸°ë³¸ ì˜ˆì™¸ ì—”ì§„ êµ¬ì¡°

### **Week 2: ê·œì¹™ í™•ì¥**
- ì¶”ê°€ ì˜¤ë¥˜ ê·œì¹™ 20ê°œ êµ¬í˜„
- ì˜ˆì™¸ ê·œì¹™ 30ê°œ êµ¬í˜„
- ìºì‹± ì‹œìŠ¤í…œ êµ¬ì¶•

### **Week 3: í†µí•© ë° ìµœì í™”**
- ë‚˜ë¨¸ì§€ ê·œì¹™ ì™„ì„±
- ë³‘ë ¬ ì²˜ë¦¬ êµ¬í˜„
- ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”

### **Week 4: ê²€ì¦ ë° ë°°í¬**
- 100ê¶Œ ì†Œì„¤ í…ŒìŠ¤íŠ¸
- ì •í™•ë„ ì¸¡ì • (ì¸ê°„ í‰ê°€ vs ì‹œìŠ¤í…œ)
- ì˜¤íƒë¥  ê°œì„ 
- ìš´ì˜ í™˜ê²½ ë°°í¬

---

## ğŸ’¡ **í•µì‹¬ í†µì°°**

1. **"ì˜ˆì™¸ëŠ” ì¼ë°˜ ê·œì¹™ì˜ ë³€í˜•ì´ ì•„ë‹˜"** â†’ ì™„ì „ ë¶„ë¦¬ëœ ì‹œìŠ¤í…œìœ¼ë¡œ ì„¤ê³„
2. **"ì„¸ê³„ê´€ì€ ì´ˆë°˜ë¶€ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥"** â†’ 1-3ì¥ ì§‘ì¤‘ ë¶„ì„ìœ¼ë¡œ 80% ê·œì¹™ íŒŒì•…
3. **"ê·œì¹™ ê¸°ë°˜ + ë¨¸ì‹ ëŸ¬ë‹ í•˜ì´ë¸Œë¦¬ë“œ"** â†’ ê·œì¹™ìœ¼ë¡œ ê¸°ë³¸, MLë¡œ ë¯¸ì„¸ ì¡°ì •
4. **"íˆ¬ëª…í•œ ì¶”ë¡  ê³¼ì •"** â†’ ëª¨ë“  íŒë‹¨ì— ê·¼ê±° ê¸°ë¡ (ì„¤ëª… ê°€ëŠ¥ AI)

ì´ ë°©ì‹ìœ¼ë¡œ **60ê°œ ì˜¤ë¥˜ + 60ê°œ ì˜ˆì™¸**ë¥¼ **ì´ˆë‹¹ 10,000ì ì²˜ë¦¬ ì†ë„**ë¡œ ë¶„ì„ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


